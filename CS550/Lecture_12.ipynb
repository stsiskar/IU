{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Artificial Inteligence (CS550)**\n",
    "<br>\n",
    "Title: **Lecture 12: Sequential Data Analysis**\n",
    "<br>\n",
    "Speaker: **Dr. Shota Tsiskaridze**\n",
    "\n",
    "Bibliography: [1] Christopher M. Bishop, *Pattern Recognition and Machine Learning*, Springer, 2006."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1 align=\"center\">Sequential Data Analysis</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3 align=\"center\">Sequential Data</h3>\n",
    "\n",
    "- **Sequential Data** is any kind of data where the **order matters**.\n",
    "\n",
    "\n",
    "- **Sequential Data** often arise through measurement of **time series**, for example:\n",
    "  - the **rainfall measurements**;\n",
    "  - the daily values of a **currency exchange rate**;\n",
    "  - the acoustic features at successive time frames used for **speech recognition**.\n",
    "  \n",
    "<img src=\"images/L12_Sequential_Data.png\" width=\"400\" alt=\"Example\" />\n",
    "\n",
    "\n",
    "- There are **two types** of **sequential distributions**:\n",
    "  - **stationary sequential distributions**, when the data evolves in time, but the distribution from which it is generated remains the same.\n",
    "  - **nonstationary sequential distributions**, when the generative distribution itself is evolving with time.\n",
    "- We shall focus on the stationary case only."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<h3 align=\"center\">Markov Models</h3>\n",
    "\n",
    "- The **easiest way** to treat **sequential data** would be simply to ignore the sequential aspects and **treat the observations** as **i.i.d.**.\n",
    "\n",
    "  However, would fail to exploit the sequential patterns in the data, such as correlations between observations that are close in the sequence.\n",
    "\n",
    "\n",
    "- For example, suppose we observe a **binary variable** denoting whether on a particular day it **rained or not**.\n",
    "\n",
    "  Given a time series of recent observations of this variable, we **wish to predict whether it will rain on the next day**.\n",
    "  \n",
    "  If we **treat the data as i.i.d.**, then the **only information** we can glean from the data is the **relative frequency of rainy days**.\n",
    "  \n",
    "  However, we know in practice that the weather often exhibits trends that may last for several days.\n",
    "  \n",
    "  Observing whether or not it rains today is therefore of significant help in predicting if it will rain tomorrow.\n",
    "  \n",
    "  \n",
    "- To **express such effects** in a probabilistic model is to consider a **Markov model**.\n",
    "\n",
    "\n",
    "- Lets use he product rule to express the joint distribution for a sequence of observations in the form:\n",
    "\n",
    "  $$p(\\mathbf{x}_1, ..., \\mathbf{x}_N ) = \\prod_{n=1}^{N} p(\\mathbf{x}_n | \\mathbf{x}_1, ..., \\mathbf{x}_{n-1}).$$\n",
    "  \n",
    "\n",
    "- If we now assume that **each of the conditional distributions** on the right-hand side\n",
    "is **independent of all previous observations except the most recent**, we obtain the **first-order Markov chain**:\n",
    "\n",
    "  $$p(\\mathbf{x}_1, ..., \\mathbf{x}_N ) = p(\\mathbf{x}_1) \\prod_{n=2}^{N} p(\\mathbf{x}_n | \\mathbf{x}_{n-1}).$$\n",
    "\n",
    "  <img src=\"images/L12_MC1.png\" width=\"600\" alt=\"Example\" />\n",
    "\n",
    "\n",
    "- If we allow the **predictions** to **depend also on the previous-but-one value**, we obtain a **second-order Markov chain**:\n",
    "\n",
    "  $$p(\\mathbf{x}_1, ..., \\mathbf{x}_N ) = p(\\mathbf{x}_1) p(\\mathbf{x}_2| \\mathbf{x}_1) \\prod_{n=3}^{N} p(\\mathbf{x}_n | \\mathbf{x}_{n-1}, \\mathbf{x}_{n-2}).$$\n",
    "\n",
    "  <img src=\"images/L12_MC2.png\" width=\"600\" alt=\"Example\" />\n",
    "\n",
    "\n",
    "- We can similarly **consider** extensions to an **$M^{th}$ order Markov chain** in which the **conditional distribution** for a particular variable **depends on the previous $M$ variables**.\n",
    "\n",
    "\n",
    "- **Note**, that we have **paid a price** for this increased flexibility because the **number of parameters** in the model is now **much larger**:\n",
    "\n",
    "  Suppose the observations are discrete variables having $K$ states. Then:\n",
    "  \n",
    "    - the conditional distribution $p(\\mathbf{x}_n| \\mathbf{x}_{n-1})$ in a **first-order Markov chain** will be specified by a set of $K − 1$ parameter for each of the $K$ states of $\\mathbf{x}_{n-1}$ giving a total of $K(K − 1)$ parameters.\n",
    "    - the conditional distribution $p(\\mathbf{x}_n| \\mathbf{x}_{n-M, ..., \\mathbf{x}_{n-1}})$ in am **$M^{th}$-order Markov chain** will be specified by a set of $K^{M-1}(K − 1)$ parameter.\n",
    "    \n",
    "  Therefore, number of parameters grows exponentially with $M$.\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<h3 align=\"center\">State Space Model</h3>\n",
    "\n",
    "Suppose we **wish to build a model** for sequences that is **not limited by the Markov assumption** to **any order** and yet that **can be specified** using a **limited number of free parameters**.\n",
    "\n",
    "\n",
    "We can achieve this by **introducing additional latent variables** to permit a rich class of models to be constructed out of simple components.\n",
    "\n",
    "- For each observation $\\mathbf{x}_n$, we introduce a corresponding latent variable $\\mathbf{z}_n$.\n",
    "- Assume that it is the **latent variables*- $\\mathbf{z}_m$ that **form a Markov chain**, giving rise to the graphical structure known as a **State Space Model**.\n",
    "- The joint distribution for this model is given by:\n",
    "\n",
    "  $$p(\\mathbf{x}_1, ..., \\mathbf{x}_N, \\mathbf{z}_1, ..., \\mathbf{z}_N ) = p(\\mathbf{z}_1) \\left [ \\prod_{n=2}^{N} p(\\mathbf{z}_n| \\mathbf{z}_{n-1}) \\right ]  \\prod_{n=1}^{N} p(\\mathbf{x}_n | \\mathbf{z}_n).$$\n",
    "  \n",
    "  <img src=\"images/L12_SSM.png\" width=\"600\" alt=\"Example\" />\n",
    "  \n",
    "  \n",
    "- There are **two important models** for sequential data that are described by this graph:\n",
    "  - If the **latent variables are discrete**, then we obtain the **Hidden Markov Model (HMM)** (Elliott et al., 1995);\n",
    "  - If **both the latent** and the **observed variables are Gaussian**, then we obtain the **Linear Dynamical System**.\n",
    "\n",
    "\n",
    "- Example of illustration of sampling from a **hidden Markov** model having a **3-state latent variable $\\mathbf{z}$** and a Gaussian emission model $p(\\mathbf{x|z})$ where $\\mathbf{x}$ is $2$-dimensional is shown below.\n",
    "\n",
    "  <img src=\"images/L12_HMM.png\" width=\"800\" alt=\"Example\" />\n",
    "\n",
    "\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<h3 align=\"center\">Kalman Filter</h3>\n",
    "\n",
    "- Suppose we **wish to measure the value of an unknown quantity** $\\mathbf{z}$ using a **noisy sensor** that returns an observation $\\mathbf{x}$ representing the value of $\\mathbf{z}$ plus **zero-mean Gaussian noise**.\n",
    "\n",
    "- We can assume the model has **linear-Gaussian conditional distributions**, so we can write the distributions in the general form:\n",
    "\n",
    "  $$p(\\mathbf{z}_n, \\mathbf{z}_{n-1}) = \\mathcal{N}(\\mathbf{z}_n | \\mathbf{Az}_{n-1}, \\Gamma),$$\n",
    "  $$p(\\mathbf{x}_n, \\mathbf{z}_{n}) = \\mathcal{N}(\\mathbf{x}_n | \\mathbf{Cz}_{n}, \\Sigma),$$\n",
    "  $$p(\\mathbf{z}_1) = \\mathcal{N}(\\mathbf{z}_1 | \\mu_0, \\mathbf{V}_0).$$\n",
    "  \n",
    "- In terms of noisy linear equations, we can express these distributions in an equivalent form:\n",
    "\n",
    "  $$\\mathbf{z}_n = \\mathbf{Az}_{n-1} + \\mathbf{w}_n,$$\n",
    "  $$\\mathbf{x}_n = \\mathbf{Cz}_{n} + \\mathbf{v}_n,$$\n",
    "  $$\\mathbf{z}_1 = \\mathbf{\\mu}_{0} + \\mathbf{u}.$$\n",
    "  \n",
    "  where the noise terms have the distributions:\n",
    "  \n",
    "  $$\\mathbf{w} \\sim \\mathcal{N}(\\mathbf{w} | 0, \\Gamma),$$\n",
    "  $$\\mathbf{v} \\sim \\mathcal{N}(\\mathbf{v} | 0, \\Sigma),$$\n",
    "  $$\\mathbf{u} \\sim \\mathcal{N}(\\mathbf{u} | 0, \\mathbf{V}_0),$$\n",
    "  \n",
    "  \n",
    "- The parameters of the model $\\theta = \\{ \\mathbf{A}, \\Gamma, \\mathbf{C}, \\Sigma, \\mu_0, \\mathbf{V}_0\\}$ can be **determined** using **maximum likelihood** through the **EM algorithm**.\n",
    "\n",
    "\n",
    "- The **problem of finding the marginal distributions for the latent variables** conditional on the observation sequence **can be solved** efficiently **using the sum-product algorithm**, which **in the context of the linear dynamical system** gives rise to the **Kalman filter**.\n",
    "\n",
    "\n",
    "- Thus we can view the **Kalman filter** as a **process of making successive predictions** and then **correcting these predictions in the light of the new observations**.\n",
    "\n",
    "  <img src=\"images/L12_Kalman_Filter.png\" width=\"1000\" alt=\"Example\" />\n",
    "\n",
    "\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3 align=\"center\">Basic Sampling Algorithms</h3>\n",
    "\n",
    "- Lets consider some simple strategies for **generating random samples from a given distribution**.\n",
    "\n",
    "- We first consider how to **generate random numbers** from simple **nonuniform distributions**, **assuming** that we **already have** available a source of **uniformly distributed random numbers**.\n",
    "\n",
    "\n",
    "- Lets suppose that $z$ is uniformly distributed over the interval $(0, 1)$ and that we transform the values of $z$ using some function $f(\\cdot)$ so that $y = f(z)$. The distribution of $y$ will be governed by:\n",
    "\n",
    "  $$p(y) = p(z) \\frac{|dz|}{|dy|},$$\n",
    "  \n",
    "  where in our case, $p(z)=1$.\n",
    " \n",
    "\n",
    "- Our goal is to choose the function $f(z)$ such that the resulting values of $y$ have some specific desired distribution $p(y)$:\n",
    "\n",
    "  $$z = h(y) = \\int_{-\\infty}^{y} p({y}')d{y}',$$\n",
    "  \n",
    "  which is the indefinite integral of $p(y)$.\n",
    "  \n",
    "  Thus:\n",
    "  \n",
    "  $$y = h^{-1}(z).$$\n",
    "  \n",
    "  <img src=\"images/L12_Sampling.png\" width=\"400\" alt=\"Example\" />\n",
    "  \n",
    "  \n",
    "- **Example**: Lets consider the **exponential distribution**:\n",
    "\n",
    "  $$p(y) = \\lambda e^{-\\lambda y},$$\n",
    "  \n",
    "  where $0 \\leq y < \\infty$.\n",
    "  \n",
    "  In this case the lower limit of the integral in is $0$, so:\n",
    "  \n",
    "  $$h(y) = 1 - e^{-\\lambda y}.$$\n",
    "  \n",
    "  Thus, if we transform our uniformly distributed variable $z$ using:\n",
    "  \n",
    "  $$y = -\\lambda^{-1} \\ln(1-z),$$\n",
    "  \n",
    "  then $y$ will have an exponential distribution.\n",
    "  \n",
    "  \n",
    "- The **generalization to multiple variables** is straightforward and involves the **Jacobian** of the change of variables, so that:\n",
    "\n",
    "  $$p(y_1, ..., y_M) = p(z_1, ..., z_M) \\left | \\frac{\\partial(z_1, ..., z_M)}{\\partial(y_1, ..., y_M)}\\right |.$$\n",
    "\n",
    "\n",
    "- **Note**: \n",
    "\n",
    "  The **transformation technique** **depends** for its success on the **ability to calculate and then invert** the indefinite integral of the required distribution. \n",
    "\n",
    "  Such operations will only be feasible for a limited number of simple distributions, and so we must turn to alternative approaches in search of a more general strategy, for example **rejection sampling**.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3 align=\"center\">Rejection Sampling</h3>\n",
    "\n",
    "-  Suppose that we are easily able to evaluate $p(z)$ for any given value of $z$, up to some normalizing constant $Z$, so that:\n",
    "\n",
    "  $$p(z) = \\frac{1}{Z_p}\\widetilde{p}(z),$$\n",
    "  \n",
    "  where $\\widetilde{p}(z)$ can readily be evaluated, but $Z_p$ is unknown.\n",
    "  \n",
    "\n",
    "- In order to apply **rejection sampling**, we need some simpler distribution $q(z)$, sometimes called a **proposal distribution**, from which we can readily draw samples.\n",
    "\n",
    "- We next introduce a **constant $k$** whose value is chosen such that:\n",
    "\n",
    " $$kq(z) \\geq p(z)$$\n",
    " \n",
    " for all values of $z$. The function $kq(z)$ is called the **comparison function** and is shown on the figure below.\n",
    "\n",
    "  <img src=\"images/L12_Rejection_Sampling.png\" width=\"500\" alt=\"Example\" />\n",
    "  \n",
    "  \n",
    "- The **rejection sampler** involves following **steps**:\n",
    "  - First, we generate a number $z_0$ from the distribution $q(z)$.\n",
    "  - Next, we generate a number $u_0$ from the **uniform distribution** over $[0, kq(z0))]$.\n",
    "  - If $u0 > \\widetilde{p}(z_0)$ then the sample is **rejected**, otherwise $u_0$ is **retained**.\n",
    "\n",
    "- The original values of $z$ are generated from the distribution $q(z)$, and these samples are then accepted with probability $\\widetilde{p}(z)/kq(z)$, and so the probability that a sample will be accepted is given by:\n",
    "\n",
    "  $$p(\\mathrm{accept}) = \\int\\frac{\\widetilde{p}(z)}{kq(z)} q(z)dz = \\frac{1}{k}\\int \\widetilde{p}(z)dz.$$\n",
    "  \n",
    "  Thus the **fraction of points** that are **rejected** by this method depends on the **ratio of the area under the unnormalized distribution $\\widetilde{p}(z)$** to the **area under the curve $kq(z)$**.\n",
    "  \n",
    "  \n",
    "- **Example**: Lets consider the **gamma distribution**:\n",
    "\n",
    "  $$\\mathrm{Gam}(z|a,b) = \\frac{b^a z^{a-1} e^{-bz}}{\\Gamma(a)}$$\n",
    "  \n",
    "  which, for $a > 1 $ has a bell-shaped form:\n",
    "\n",
    "  <img src=\"images/L12_Gamma.png\" width=\"500\" alt=\"Example\" />\n",
    "\n",
    "  A **suitable proposal distribution** is therefore the **Cauchy** because this too is bell-shaped:\n",
    "  \n",
    "  $$q(z) = \\frac{k}{1 + \\frac{(z-c)^2}{b^2}}.$$\n",
    "  \n",
    "  The minimum reject rate is obtained by setting $c= a -1$ and $b2 = 2a-1$, choosing the constant $k$ to be as small as possible while still satisfying the requirement $kq(z) \\geq  \\widetilde{p}(z)$.\n",
    "  \n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3 align=\"center\">Adaptive Rejection Sampling</h3>\n",
    "\n",
    "- In many instances it proves **difficult** to determine a **suitable** analytic form for the envelope **distribution $q(z)$**.\n",
    "\n",
    "\n",
    "- An **alternative approach** is to **construct** the envelope function on the **fly based on measured values** of the **distribution $p(z)$**.\n",
    "\n",
    "- **Construction** of an envelope function is particularly **straightforward** when $\\ln p(z)$ has derivatives that are nonincreasing functions of $z$.\n",
    "\n",
    "  <img src=\"images/L12_Adaptive.png\" width=\"500\" alt=\"Example\" />\n",
    "\n",
    "  \n",
    "- For **adaptive rejection sampling**:\n",
    "  - First the function $\\ln p(z)$ and its **gradient** are evaluated at some **initial set of grid** points;\n",
    "  - Next the **intersections of the resulting tangent lines** are **used** to construct the **envelope function**;\n",
    "  - **Then** the **usual rejection criterion** can be **applied**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3 align=\"center\">Monte Carlo Integration</h3>\n",
    "\n",
    "- Suppose you want to calculate a certain integral:\n",
    "\n",
    "  $$\\int_{a}^{b} f(x) dx.$$\n",
    "  \n",
    "\n",
    "- Lets consider a **random variable** $y$ **uniformly distributed** over the integration interval $[a,b]$.\n",
    " \n",
    "  Then, $f(y)$ will also be a random variable, and its mathematical expectation is expressed as:\n",
    "  \n",
    "  $$\\mathbb{E}f(y) = \\int_{a}^{b} f(zx)p(x)dx.$$\n",
    "  \n",
    "  where $p(x)$ is the **distribution density** of the random variable $y$ equal to $\\frac{1}{b-a}$.\n",
    "  \n",
    "  Thus, the desired integral is expressed as:\n",
    "  \n",
    "  $$\\int_{a}^{b} f(x)dx = (b-a) \\mathbb{E} f(y).$$\n",
    "  \n",
    "- The **expectation** of the **random variable $f (y)$** can be easily estimated by modeling this random variable and **calculating the sample mean**.\n",
    "  \n",
    "  We throw $N$ points uniformly distributed on interval $[a,b]$ and for each $y_i$ we calculate $f(y_i)$.\n",
    "  \n",
    "  Next we calculate the sample average:\n",
    "  \n",
    "  $$\\frac{1}{N} \\sum_{i=1}^{N}f(y_i).$$\n",
    "  \n",
    "  Therefore we get:\n",
    "  \n",
    "  $$\\int_{a}^{b} f(x)dx \\approx  \\frac{b-a}{N}\\sum_{i=1}^{N} f(y_i)$$\n",
    "  \n",
    "- The accuracy of the estimate depends only on the number of points N."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3 align=\"center\">Monte Carlo Geometric Integration Algorithm</h3>\n",
    "\n",
    "To determine the area under the function graph, we can use the following stochastic algorithm:\n",
    "\n",
    "- First, we **restrict the function** to a **rectangle** (an $n$-dimensional parallelepiped in the case of many dimensions), whose area $S_ {par}$ can be easily calculated; \n",
    "- Next, we **sketch**” into this **rectangle (box)** a certain number of **points** ($N$ pieces), the coordinates of which we will randomly select;\n",
    "- Define the number of points ($K$ pieces) that fall under the function graph;\n",
    "- The area of the region bounded by the function and coordinate axes, $S$ is given by the expression:\n",
    "\n",
    " $$S = S_ {par} {\\frac {K} {N}}.$$\n",
    "\n",
    "\n",
    "- For a **small number of measurements** of an integrable function, the **Monte Carlo integration performance** is **much lower** than the **performance of deterministic methods**. \n",
    "\n",
    "- **Nevertheless**, in some cases, when the **function is specified implicitly**, and it is **necessary to determine the domain** specified in the form of complex inequalities, the **stochastic method may be preferable**.\n",
    "\n",
    "\n",
    "\n",
    "  <img src=\"images/L12_MC.png\" width=\"500\" alt=\"Example\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "<h1 align=\"center\">End of Lecture</h1>"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
