{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Artificial Inteligence (CS550)**\n",
    "<br>\n",
    "Title: **Lecture 7: Logistic Regression**\n",
    "<br>\n",
    "Speaker: **Dr. Shota Tsiskaridze**\n",
    "\n",
    "Bibliography: [1] Christopher M. Bishop, *Pattern Recognition and Machine Learning*, Springer, 2006."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1 align=\"center\">Logistic Regression</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3 align=\"center\">Linear Basis Function Models</h3>\n",
    "\n",
    "The simplest linear model for regression is one that involves a linear combination of\n",
    "the input variables:\n",
    "\n",
    "$$y(\\mathbf{x}, \\mathbf{w}) = w_0 + w_1 x_1 + ... + w_D x_D,$$\n",
    "\n",
    "where $\\mathbf{x} = (x_1, ..., x_D)^{\\mathbf{T}}$. The key property of this model is that it is a linear function of the parameters $w_0, ..., w_D$.\n",
    "\n",
    "We can extend the class of models by con|sidering linear combinations of fixed nonlinear functions of the input variables, of the form:\n",
    "\n",
    "$$y(\\mathbf{x}, \\mathbf{w}) = w_0 + \\sum_{j=1}^{M-1} w_j \\phi_j(\\mathbf{x}) = \\mathbf{w}^{\\mathbf{T}} \\mathbf{\\phi}(\\mathbf{x}),$$\n",
    "\n",
    "where $\\phi_j(\\mathbf{x})$ are known as **basis functions**, $\\mathbf{w} = (w_0, ..., w_{M-1})^{\\mathbf{T}}$ and $\\mathbf{\\phi} = (\\phi_0, ..., \\phi_{M-1})^{\\mathbf{T}}.$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<h3 align=\"center\">Examples of basis functions</h3>\n",
    "\n",
    "There are many other possible choices for the basis functions:\n",
    "\n",
    "**Polinomial** basis function: $\\phi_j(x) = x^j.$\n",
    "\n",
    "**Gaussian** basis function: $\\phi_j(x) = \\textrm{exp} \\left\\{ -\\frac{(x-\\mu_j)^2}{2s^2} \\right\\}.$\n",
    "\n",
    "**Sigmoidal** basis function: $\\phi_j(x) = \\sigma \\left ( \\frac{x-\\mu_j}{s} \\right ),$ where $\\sigma(a)$ is the **logistic sigmoid function** defined by $\\sigma(a) = \\frac{1}{1 + \\textrm{exp}(-a)}.$\n",
    "\n",
    "**Fourier** basis function:  $\\phi_0(x) = w_0 $, $\\phi_{2k+1}(x) = \\cos(n x)$, $\\phi_{2k}(x) = \\sin(n x)$,\n",
    "\n",
    "In many signal processing applications, it is of interest to consider basis functions that are localized in both space and frequency, leading to a class of functions known as **wavelets**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<h3 align=\"center\">Examples of basis functions</h3>\n",
    "\n",
    "<img src=\"images/L7_basis_functions.png\" width=\"1800\" alt=\"Example\" />\n",
    "\n",
    "Figure showing **polynomials** (on the left), **Gaussians** (in the centre), and **sigmoidal** (on the right) basis functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3 align=\"center\">The Gaussian Distribution</h3>\n",
    "\n",
    "In the case of a **single variable** $x$, the **Gaussian distribution**, also known as the **normal distribution**, can be written in the form:\n",
    "\n",
    "$$\\mathcal{N}(x | \\mu, \\sigma^2) = \\frac{1}{{\\left ( 2\\pi \\sigma^2 \\right )}^{1/2}}\\textrm{exp}\\left\\{ -\\frac{1}{2\\sigma^2} \\left( x-\\mu\\right)^2\\right\\},$$\n",
    "\n",
    "where $\\mu$ is the **mean** and $\\sigma^2$ is the variance.\n",
    "\n",
    "For a $D$-dimensional vector $\\textbf{x}$, the **multivariate Gaussian distribution** takes the form:\n",
    "\n",
    "$$\\mathcal{N}(\\textbf{x} | \\mathbf{\\mu}, \\mathbf{\\Sigma}) = \\frac{1}{{\\left ( 2\\pi\\right )}^{D/2}} \\frac{1}{{\\left | \\Sigma \\right |}^{1/2}} \\textrm{exp}\\left\\{ -\\frac{1}{2}\\left( \\textbf{x} - \\mu \\right)^{\\textbf{T}} \\Sigma^{-1} \\left( \\textbf{x} - \\mu \\right)\\right\\},$$\n",
    "\n",
    "where $\\mu$ is a $D$-dimensional mean vector, $\\mathbf{\\Sigma}$ is a $D \\times D$ covariance matrix, and $|\\mathbf{\\Sigma}|$ denotes the determinant of $\\mathbf{\\Sigma}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3 align=\"center\">Maximum Likelihood and least squares</h3>\n",
    "\n",
    "Lets show that the **least squares approach** could be motivated as the **maximum likelihood solution** under an assumed **Gaussian noise** model.\n",
    "\n",
    "Let assume that the target variable $t$ is given by by a deterministic function with additive **Gaussian noise** so that:\n",
    "\n",
    "$$t = y( \\mathbf{x}, \\mathbf{w}) + \\epsilon,$$\n",
    "where $\\epsilon$ is a zero mean Gaussian random variable with precision $\\beta = \\frac{1}{\\sigma}$.\n",
    "\n",
    "Thus we can write:\n",
    "\n",
    "$$p(t | \\mathbf{x}, \\mathbf{x}, \\beta) = \\mathcal{N} (t | y( \\mathbf{x}, \\mathbf{w}), \\beta^{-1}).$$\n",
    "\n",
    "In the case of a Gaussian conditional distribution the conditional mean of the target variable $t$:\n",
    "\n",
    "$$\\mathbb{E}[t|\\mathbf{x}] = \\int tp(t | \\mathbf{x}) dt = y(\\mathbf{x}, \\mathbf{w}).$$\n",
    "\n",
    "Now lets consider a data set of inputs $\\mathbf{X} = \\{\\mathbf{x_1}, ..., \\mathbf{x_N}\\}$ with corresponding target\n",
    "values $\\mathbf{t} = \\{t_1, ..., t_N\\}$.\n",
    "Making the assumption that these data points are **drawn independently** we obtain the following expression for the likelihood function:\n",
    "\n",
    "$$p(\\mathbf{t} | \\mathbf{X}, \\mathbf{w}, \\beta) = \\prod_{n=1}^{N} \\mathcal{N} (t_n | \\mathbf{w}^{\\mathbf{T}}\\mathbf{\\phi}(\\mathbf{x_n}), \\beta^{-1}).$$\n",
    "\n",
    "Taking the logarithm of the likelihood function, and taking the standard form for the univariate Gaussian, we have:\n",
    "\n",
    "$$\\ln p(\\mathbf{t} | \\mathbf{X}, \\mathbf{w}, \\beta) = \n",
    "\\sum_{n=1}^{N} \\ln \\mathcal{N} (t_n | \\mathbf{w}^{\\mathbf{T}}\\mathbf{\\phi}(\\mathbf{x_n}), \\beta^{-1}) = \n",
    "\\frac{N}{2} \\ln \\beta  - \\frac{N}{2} \\ln (2\\pi)  - \\beta E_D(\\mathbf{w}),$$\n",
    "\n",
    "where the sum-of-squares error function is defined by:\n",
    "\n",
    "$$E_D(\\mathbf{w}) = \\frac{1}{2} \\sum_{n=1}^{N}\\left\\{  t_n - \\mathbf{w}^{\\mathbf{T}}\\mathbf{\\phi}(\\mathbf{x_n})\\right\\}^2.$$\n",
    "\n",
    "We see that **maximization of the likelihood** function **under a conditional Gaussian noise** distribution for a linear model is equivalent to **minimizing a sum-of-squares error** function given by $E_D(\\mathbf{w})$.\n",
    "\n",
    "The gradient of the log likelihood function takes the form:\n",
    "\n",
    "$$\\nabla \\ln p(\\mathbf{t} | \\mathbf{X}, \\mathbf{w}, \\beta) = \n",
    "\\sum_{n=1}^{N}\\left\\{  t_n - \\mathbf{w}^{\\mathbf{T}}\\mathbf{\\phi}(\\mathbf{x_n})\\right\\} \\mathbf{\\phi}(\\mathbf{x_n})^{\\mathbf{T}}.$$\n",
    "\n",
    "Setting this gradient to zero gives:\n",
    "\n",
    "$$ 0 = \\sum_{n=1}^{N} t_n \\mathbf{\\phi}(\\mathbf{x_n})^{\\mathbf{T}} - \\mathbf{w}^{\\mathbf{T}} \\left ( \\sum_{n=1}^{N} \\mathbf{\\phi}(\\mathbf{x_n})\\mathbf{\\phi}(\\mathbf{x_n})^{\\mathbf{T}} \\right ).$$\n",
    "\n",
    "Solving for $\\mathbf{w}$ we obtaing:\n",
    "\n",
    "$$\\mathbf{w}_{ML}  = \\left ( \\Phi^{\\mathbf{T}} \\Phi \\right )^{-1} \\Phi^{\\mathbf{T}} \\mathbf{t},$$\n",
    "\n",
    "which are known as the **normal equations** for the **least squares problem**.\n",
    "\n",
    "Here $\\Phi$ is an $N \\times M$ matrix, called the **design matrix**, whose elements are given by $\\Phi_{nj} = \\phi_j(\\mathbf{x_n})$, so that:\n",
    "\n",
    "$$\\Phi = \\begin{bmatrix}\n",
    " \\phi_0(\\mathbf{x_1}) & \\phi_1(\\mathbf{x_1}) & \\cdots & \\phi_{M-1}(\\mathbf{x_1})  \\\\ \n",
    " \\phi_0(\\mathbf{x_2}) & \\phi_1(\\mathbf{x_2}) & \\cdots & \\phi_{M-1}(\\mathbf{x_2})  \\\\ \n",
    "\\vdots  & \\vdots  & \\ddots  & \\vdots \\\\ \n",
    "\\phi_0(\\mathbf{x_N}) & \\phi_1(\\mathbf{x_N}) & \\cdots & \\phi_{M-1}(\\mathbf{x_N})\n",
    "\\end{bmatrix}.$$\n",
    "\n",
    "The quantity:\n",
    "\n",
    "$$\\Phi^{\\dagger} \\equiv \\left ( \\Phi^{\\mathbf{T}} \\Phi \\right )^{-1} \\Phi^{\\mathbf{T}}$$\n",
    "\n",
    "is known as the **Moore-Penrose pseudo-inverse** of the matrix $\\Phi$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3 align=\"center\">Batch Techniques and Sequential Learning</h3>\n",
    "\n",
    "- **Batch techniques** involve processing the entire training set in **one go**.<br>\n",
    "  **Example**: Maximum Likelihood solution. Can be computationally costly for large data sets.\n",
    "  \n",
    "  \n",
    "- **Sequential algorithms** consider the data points one at a time and the model parameters updated after each such presentation.<br>\n",
    "  **Example**: Stochastic Gradient Descent (SGD):  \n",
    "  $$\\mathbf{w}^{(\\tau + 1)} = \\mathbf{w}^{(\\tau)} - \\eta \\nabla E_n, $$\n",
    "  where $\\tau$ denotes the iteration number, and $\\eta$ is a learning rate parameter.\n",
    "\n",
    "\n",
    "- For the case of the **sum-of-squares error** function, this gives:\n",
    "  $$\\mathbf{w}^{(\\tau + 1)} = \\mathbf{w}^{(\\tau)} + \\eta (t_n  - {\\mathbf{w}^{(\\tau)}}^{\\mathbf{T}}\\phi(\\mathbf{x_n}))\\phi(\\mathbf{x_n}),$$\n",
    "  which is known as **least-mean-squares** or the **LMS algorithm**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3 align=\"center\">Multiple Outputs</h3>\n",
    "\n",
    "So far, we have considered the case of a **single target variable** $t$.\n",
    "<br>\n",
    "In some applications, we may wish to predict $K > 1$ target variables.\n",
    "\n",
    "This could be done by introducing a different set of basis functions for each component of $t$, \n",
    "<br> leading to multiple, independent regression problems $\\rightarrow$ **Not interesting**.\n",
    "\n",
    "More common approach is to use the same set of basis functions:\n",
    "\n",
    "$$y(\\mathbf{x}, \\mathbf{w}) = \\mathbf{W}^{\\mathbf{T}}\\phi(\\mathbf{x}),$$\n",
    "\n",
    "where $y$ is a $K$-dimensional column vector, $\\mathbf{W}$ an $M \\times K$ matrix of parameters,\n",
    "and $\\phi(\\mathbf{x})$ is an $M$-dimensional column vector with elements $\\phi_j(\\mathbf{x})$.\n",
    "\n",
    "Suppose we take the conditional distribution of the target vector to be an isotropic Gaussian of the form:\n",
    "\n",
    "$$p(\\mathbf{t} | \\mathbf{x}, \\mathbf{W}, \\beta) = \\mathcal{N} \\left ( \\mathbf{t} | \\mathbf{W}^{\\mathbf{T}} \\phi(\\mathbf{x}), \\beta^{-1}\\mathbf{I} \\right ).$$\n",
    "\n",
    "If we have a set of observations ${\\mathbf{t_1}, ..., \\mathbf{t_N}}$, we can combine these into a matrix $\\mathbf{T}$ of size $N \\times K$.\n",
    "<br>\n",
    "Similarly, we can combine the input vectors ${\\mathbf{x_1}, ..., \\mathbf{x_N}}$ into a matrix $\\mathbf{X}$ of size $N \\times M$.\n",
    "\n",
    "The **log likelihood function** is then given by:\n",
    "\n",
    "$$\\ln p(\\mathbf{T | \\mathbf{X}, \\mathbf{W}, \\beta}) = \n",
    "\\sum_{n=1}^{N} \\ln \\mathcal{N} \\left ( \\mathbf{t_n} | \\mathbf{W}^{\\mathbf{T}} \\phi(\\mathbf{x_n}), \\beta^{-1}\\mathbf{I}\\right ) = \\frac{NK}{2} \\ln \\left( \\frac{\\beta}{ 2\\pi}\\right)  - \\frac{\\beta}{2} \\sum_{n=1}^{N} \\left \\| \\mathbf{t_n} - \\mathbf{W}^{\\mathbf{T}} \\phi(\\mathbf{x_n}) \\right \\|^2.$$\n",
    "\n",
    "As before, we can maximize this function with respect to $\\mathbf{W}$, giving:\n",
    "\n",
    "$$\\mathbf{W}_{ML} = \\left ( \\Phi^{\\mathbf{T}} \\Phi \\right )^{-1} \\Phi^{\\mathbf{T}} \\mathbf{T}.$$\n",
    "\n",
    "If we examine this result for each target variable $t_k$, we have:\n",
    "\n",
    "$$\\mathbf{w}_k = \\left ( \\Phi^{\\mathbf{T}} \\Phi \\right )^{-1} \\Phi^{\\mathbf{T}} \\mathbf{t}_k = \\Phi^{\\dagger} \\mathbf{t}_k.$$\n",
    "\n",
    "Thus the solution to the regression problem **decouples** between the different target variables, and **we need only compute a single pseudo-inverse matrix** $\\Phi^{\\dagger}$, which is shared by all of the vectors $\\mathbf{w}_k$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3 align=\"center\">Prior and Posterior Probabilities</h3>\n",
    "\n",
    "Lets assume that we want to make inferences about quantities of the feature parameters $\\mathbf{w}$.\n",
    "<br>\n",
    "We capture our assumptions about $\\mathbf{w}$, **before observing the data**, in the form of a **prior probability** distribution $\\mathcal{p}(\\mathbf{w})$.\n",
    "\n",
    "The effect of the observed data $\\mathcal{D} = \\{t_1, ..., t_n\\}$ is expressed through the conditional probability $\\mathcal{p}(\\mathcal{D | \\mathbf{w}})$\n",
    "<br>\n",
    "Bayes’ theorem, which takes the form:\n",
    "$$\\mathcal{p}(\\mathbf{w} | \\mathcal{D}) = \\frac{\\mathcal{p}(\\mathcal{D} | \\mathbf{w}) \\mathcal{p}(\\mathbf{w})}{\\mathcal{p}(\\mathcal{D})},$$\n",
    "\n",
    "then allows us to evaluate the uncertainty in $\\mathbf{w}$ *after* we observe $\\mathcal{D}$ in the form of the posterior probability $\\mathcal{p}(\\mathbf{w} | \\mathcal{D})$.\n",
    "\n",
    "The quantity $\\mathcal{p}(\\mathcal{D} | \\mathbf{w})$ is evaluated for the observed data set $\\mathcal{D}$ and can be viewed as a function of the parameter vector $\\mathbf{w}$, in which case it is called the **likelihood function**.\n",
    "\n",
    "Given this definition of **likelihood**, we can state Bayes’ theorem in next words:\n",
    "\n",
    "$$\\texttt{posterior} \\propto \\texttt{likelihood} \\times \\texttt{prior}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3 align=\"center\">Classification Problems</h3>\n",
    "\n",
    "- The goal in **classification** is to take an input vector $\\mathbf{x}$ and to assign it to one of $K$ discrete classes $\\mathcal{C}_k$ where $k = 1, ..., K$. \n",
    "\n",
    "- The classes are taken to be **disjoint**, so that each input is assigned to one and only one class.\n",
    "\n",
    "- The **input space** is thereby divided into **decision regions** whose boundaries are called **decision boundaries** or **decision surfaces**.\n",
    "\n",
    "\n",
    "- There are **three distinct approaches** to the classification problem:\n",
    "  - **constructing a discriminant function**, i.e. directly assigns each vector $\\mathbf{x}$  to a specific class. (**Next Lecture!**)\n",
    "  - **directly modeling of the conditional probability$p(\\mathcal{C}_k |\\mathbf{x})$**, i.e. representing them as parametric models and then optimizing the parameters using a training set. \n",
    "  - **adopt a generative approach**, i.e. model the class-conditional densities given by $p(\\mathbf{x} |\\mathcal{C}_k)$, together with the prior probabilities $p(\\mathcal{C}_k)$ for the classes, and then compute the required posterior probabilities using Bayes’ theorem:\n",
    "  \n",
    "  $$p(\\mathcal{C}_k | \\mathbf{x}) = \\frac{p(\\mathbf{x} |\\mathcal{C}_k)p(\\mathcal{C}_k)}{p( \\mathbf{x})}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<h3 align=\"center\">Linear Models for Classification</h3>\n",
    "\n",
    "- **Linear models for classification** mean that the **decision surfaces are linear functions** of the input vector $\\mathbf{x}$ and hence are defined by $(D − 1)$-dimensional hyperplanes within the $D$-dimensional input space.\n",
    "- Data sets whose classes **can be separated** exactly by **linear decision surfaces** are said to be **linearly separable**.\n",
    "\n",
    "\n",
    "- In the **linear regression models** the model prediction $y(\\mathbf{x}, \\mathbf{w})$ was given by a linear function of the parameters $\\mathbf{w}$.\n",
    "- For **classification** problems, however, we wish to **predict discrete class** labels, or more generally **posterior probabilities** that lie in the range $(0, 1)$.\n",
    "\n",
    "- To achieve this, we consider a **generalization** of this model in which we **transform the linear function** of $\\mathbf{w}$ using a **nonlinear function** $f(\\cdot)$ so that:\n",
    "\n",
    "  $$y(\\mathbf{x}) = f \\left ( \\mathbf{w}^{\\mathbf{T}} \\mathbf{x} + w_0 \\right ).$$\n",
    "\n",
    "  In the machine learning literature $f(\\cdot)$ is known as an **activation function**.\n",
    "\n",
    "\n",
    "- The **decision surfaces** correspond to $y(\\mathbf{x}) = constant$, so that $\\mathbf{w}^{\\mathbf{T}} \\mathbf{x} + w_0 = constant$ and hence the **decision surfaces are linear functions** of $\\mathbf{x}$, even if the function $f(\\cdot)$ is nonlinear.\n",
    "\n",
    "- This class of models are called **generalized linear models**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<h3 align=\"center\">Probabilistic Generative Models</h3>\n",
    "\n",
    "-  Lets consider the case of two classes $\\mathcal{C}_1$ and $\\mathcal{C}_2$.\n",
    "\n",
    "  The **posterior probability** for class $\\mathcal{C}_1$ can be written as:\n",
    "  $$p(\\mathcal{C}_1 | \\mathbf{x}) = \n",
    "\\frac{p(\\mathbf{x} | \\mathcal{C}_1)p(\\mathcal{C}_1)}{p(\\mathbf{x} | \\mathcal{C}_1)p(\\mathcal{C}_1) + p(\\mathbf{x} | \\mathcal{C}_2)p(\\mathcal{C}_2)}\n",
    "= \\frac{1}{1 + \\textrm{exp}(-a)} = \\sigma(a),$$\n",
    "  where we have defined:\n",
    "  $$a = \\ln \\frac{p(\\mathbf{x} | \\mathcal{C}_1)p(\\mathcal{C}_1)}{p(\\mathbf{x} | \\mathcal{C}_2)p(\\mathcal{C}_2)},$$\n",
    "  and $\\sigma(a)$ is the **logistic sigmoid function** defined by:\n",
    "  $$\\sigma(a) = \\frac{1}{1 + \\textrm{exp}(-a)}.$$\n",
    "  The **inverse** of the **logistic sigmoid** is given by:\n",
    "  $$a = \\ln \\left ( \\frac{\\sigma}{ 1 - \\sigma}\\right ),$$\n",
    "  and is known as the **logit function**.\n",
    "\n",
    "\n",
    "- For the case of $K > 2$ classes, we have:\n",
    "  $$p(\\mathcal{C}_k | \\mathbf{x}) = \n",
    "\\frac{p(\\mathbf{x} | \\mathcal{C}_k)p(\\mathcal{C}_k)}{\\sum_{j} p(\\mathbf{x} | \\mathcal{C}_k)p(\\mathcal{C}_k)}\n",
    "= \\frac{\\mathrm{exp}(a_k)}{\\sum_{j} \\mathrm{exp}(a_k)},$$\n",
    "  where quantities $a_k$ are defined by:\n",
    "  $$a_k = \\ln p(\\mathbf{x} | \\mathcal{C}_k)p(\\mathcal{C}_k),$$\n",
    "  and is known as the **softmax function**.\n",
    "  \n",
    "  If $a_k \\gg a_j$ for all $j \\neq k$, then $p(\\mathbf{x} | \\mathcal{C}_k) \\simeq 1$ and $p(\\mathbf{x} | \\mathcal{C}_j) \\simeq 0.$  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<h3 align=\"center\">Continuous Inputs</h3>\n",
    "   \n",
    "- Let us assume that the class-conditional densities are Gaussian:\n",
    "\n",
    "  $$p(\\mathbf{x} | \\mathcal{C}_k) = \\frac{1}{(2\\pi)^{D/2}} \\frac{1}{|\\mathbf{\\Sigma}|^{1/2}} \\mathrm{exp} \\left \\{ -\\frac{1}{2} (\\mathbf{x} - \\mathbf{\\mu}_k)^{\\mathbf{T}} \\mathbf{\\Sigma}^{-1} (\\mathbf{x} - \\mathbf{\\mu}_k)\\right \\}.$$\n",
    "  \n",
    "- For the case of two classes we have:\n",
    "\n",
    "  $$p(\\mathcal{C}_1 | \\mathbf{x}) = \\sigma(\\mathbf{w}^{\\mathbf{T}}\\mathbf{x} + w_0).$$\n",
    "  where we have defined:\n",
    "  \n",
    "  $$\\begin{matrix}\n",
    "\\mathbf{w}  =  & \\mathbf{\\Sigma}^{-1} (\\mathbf{\\mu_1} - \\mathbf{\\mu_2}) \\\\ \n",
    "w_0 = & -\\frac{1}{2} \\mathbf{\\mu_1}^{\\mathbf{T}} \\mathbf{\\Sigma}^{-1} \\mathbf{\\mu_1}  + \\frac{1}{2} \\mathbf{\\mu_2}^{\\mathbf{T}} \\mathbf{\\Sigma}^{-1} \\mathbf{\\mu_2} + \\ln \\frac{p(\\mathcal{C_1})}{p(\\mathcal{C_2})}\n",
    "\\end{matrix}\n",
    "$$\n",
    "\n",
    "  We see that the **quadratic terms** in $\\mathbf{x}$ from the exponents of the Gaussian densities **have cancelled** leading to a **linear function** of $\\mathbf{x}$ in the argument of the **logistic sigmoid**.\n",
    "  \n",
    "\n",
    "- For the general case of $K$ classes we have:\n",
    "\n",
    "  $$a_k(\\mathbf{x}) = \\mathbf{w}_k^{\\mathbf{T}} \\mathbf{x} + w_{k_0}.$$\n",
    "  where we have defined:\n",
    "  \n",
    "   $$\\begin{matrix}\n",
    "\\mathbf{w}_k  =  & \\mathbf{\\Sigma}^{-1} \\mathbf{\\mu}_k \\\\ \n",
    "w_{k_0} = & -\\frac{1}{2} \\mathbf{\\mu_1}^{\\mathbf{T}} \\mathbf{\\Sigma}^{-1} \\mathbf{\\mu_1} + \\ln p(\\mathcal{C}_k)  \n",
    "\\end{matrix}\n",
    "$$ \n",
    "\n",
    "  We see that the $a_k(\\mathbf{x})$ are again linear functions of $\\mathbf{x}$ as a consequence of the cancellation of the quadratic terms due to the shared covariances.\n",
    "  \n",
    "  \n",
    "- The **resulting decision boundaries**, corresponding to the **minimum misclassification rate**, will occur when **two of the posterior probabilities are equal**, and so will be defined by **linear functions** of $\\mathbf{x}$, and so again we **have** a **generalized linear model**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<h3 align=\"center\">Discrete Features</h3>\n",
    "   \n",
    "- Let us now consider the case of discrete feature values $x_i$.\n",
    "\n",
    "- For simplicity, we begin by looking at binary feature values $x_i \\in \\{0,1\\}$.\n",
    "\n",
    "- If there are $D$ inputs, then a general distribution would correspond to a table of $2D$ numbers for each class, containing $2D − 1$ independent variables.\n",
    "\n",
    "- Because this grows exponentially with the number of features, we might seek a more restricted representaSection:\n",
    "\n",
    "  $$p(\\mathbf{x} | \\mathcal{C}_k) = \\prod_{i=1}^{D} \\mu_{k_i}^{x_i} (1 - \\mu_{k_i})^{1 - x_i},$$\n",
    "  \n",
    "  which contains $D$ independent parameters for each class.\n",
    "  \n",
    "  Substituting into $a_k = \\ln p(\\mathbf{x} | \\mathcal{C}_k)p(\\mathcal{C}_k)$ gives:\n",
    "  \n",
    "  $$a_k(\\mathbf{x}) = \\sum_{i=1}^{D}\\{ x_i \\ln \\mu_{k_i} + (1 - x_i) \\ln (1- \\mu_{k_i}) \\} + \\ln p(\\mathcal{C}_k),$$\n",
    "  which again are linear functions of the input values $x_i$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3 align=\"center\">Fixed Basis Functions</h3>\n",
    "\n",
    "- So far we have considered classification models that work directly with the original input vector $\\mathbf{x}$.\n",
    "\n",
    "- However, all of the algorithms are **equally applicable** if we first make a **fixed nonlinear transformation** of the inputs using a vector of basis functions $\\phi(\\mathbf{x})$.\n",
    "\n",
    "\n",
    "- The **resulting decision boundaries** can be linear in the feature space $\\phi$, and these correspond to nonlinear decision boundaries in the original $\\mathbf{x}$ space.\n",
    "\n",
    "\n",
    "- **Suitable choices** of nonlinearity can **make the process** of modelling the posterior probabilities **easier**.\n",
    "\n",
    "<img src=\"images/L7_fixed_basis_functions.png\" width=\"1800\" alt=\"Example\" />\n",
    "\n",
    "\n",
    "."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3 align=\"center\">Probabilistic Generative Models (Multiple Clases)</h3>\n",
    "   \n",
    "For the case of $K > 2$ classes, we have:\n",
    "  $$p(\\mathcal{C}_k | \\mathbf{x}) = \n",
    "\\frac{p(\\mathbf{x} | \\mathcal{C}_k)p(\\mathcal{C}_k)}{\\sum_{j} p(\\mathbf{x} | \\mathcal{C}_k)p(\\mathcal{C}_k)}\n",
    "= \\frac{\\mathrm{exp}(a_k)}{\\sum_{j} \\mathrm{exp}(a_k)},$$\n",
    "  where quantities $a_k$ are defined by:\n",
    "  $$a_k = \\ln p(\\mathbf{x} | \\mathcal{C}_k)p(\\mathcal{C}_k),$$\n",
    "  and is known as the **softmax function**.\n",
    "  \n",
    "If $a_k \\gg a_j$ for all $j \\neq k$, then $p(\\mathbf{x} | \\mathcal{C}_k) \\simeq 1$ and $p(\\mathbf{x} | \\mathcal{C}_j) \\simeq 0.$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3 align=\"center\">Logistic Regresion</h3>\n",
    "\n",
    "Lets consider generalized linear models of **two-class classification**.\n",
    "\n",
    "- The **posterior probability** of class $\\mathcal{C}_1$ can be written as a **logistic sigmoid** acting on a linear function of the feature vector $\\phi$ so that:\n",
    "\n",
    "  $$p(\\mathcal{C_1 | \\phi}) = y(\\phi) = \\sigma \\left ( \\mathbf{w}^{\\mathbf{T}}\\phi \\right ),$$\n",
    "  and\n",
    "  $$p(\\mathcal{C_2 | \\phi}) = 1 - p(\\mathcal{C_1 | \\phi}).$$\n",
    "\n",
    "\n",
    "- In the terminology of statistics, this model is known as **logistic regression**, although it should be emphasized that this is a **model for classification** rather than regression.\n",
    "\n",
    "\n",
    "- If feature space $\\phi$ is $M$-dimensional, then this model has $M$ adjustable parameters,\n",
    "<br>i.e. we have the **linear dependence** on $M$ of the number of parameters in **logistic regression**.\n",
    "\n",
    "\n",
    "- By contrast, if we had fitted Gaussian **class conditional densities** using **maximum likelihood**, we would have used $2M$ parameters for the **means** and $M(M+1)/2$ for **covariance matrix** (**symmetric!**), i.e overall $1 + M(M+5)/2$ parameters, which grows quadratically with $M$.\n",
    "\n",
    "\n",
    "- Now lets use **maximum likelihood** to determine the **parameters** of the **logistic regression** model.\n",
    "\n",
    "  For a data set $\\{ \\phi_n, t_n\\}$, where $t_n \\in {0, 1}$ and $\\phi_n = \\phi(\\mathbf{x}_n)$, with $n = 1, ..., N$, the likelihood function can be written:\n",
    "  \n",
    "  $$p(\\mathbf{t} | \\mathbf{w}) = \\prod_{n=1}^{N} y_n^{t_n}(1-y_n)^{1-t_n},$$\n",
    "  \n",
    "  where $\\mathbf{t} = (t_1, .., t_n)^{\\mathbf{T}}$ and $y_n = p(\\mathcal{C}_1 | \\phi_n).$\n",
    "\n",
    "  As usual, we can define an **error function** by taking the **negative logarithm of the likelihood**:\n",
    "  \n",
    "  $$E(\\mathbf{w}) = - \\ln p(\\mathbf{t} | \\mathbf{w}) = - \\sum_{n=1}^{N} (t_n \\ln y_n - (1 - t_n) \\ln(1 - y_n)),$$\n",
    "  \n",
    "  where $y_n = \\sigma(a_n)$ and $a_n = \\mathbf{w}^{\\mathbf{T}} \\phi_n.$\n",
    "  \n",
    "  Taking the gradient of the error function with respect to $\\mathbf{w}$, we obtain:\n",
    "  \n",
    "  $$\\nabla E(\\mathbf{w}) = \\sum_{n=1}^{N} (y_n - t_n) \\phi_n.$$\n",
    "\n",
    "  where we used the well known property of the logistic function: $\\frac{d\\sigma}{da} = \\sigma(1 - \\sigma).$\n",
    "  \n",
    "  \n",
    "- We see that the **factor involving the derivative** of the logistic sigmoid **has cancelled**, leading to a simplified form for the gradient of the log likelihood.\n",
    "\n",
    "- If desired, we could make use this result to give a **sequential algorithm** in which patterns are presented one at a time.\n",
    "\n",
    "\n",
    "- It is worth noting that maximum likelihood **can exhibit severe over-fitting** for data sets that are **linearly separable**.\n",
    "\n",
    "  This arises because the **maximum likelihood solution** **occurs** when the **hyperplane corresponding** to $\\sigma = 0.5$, equivalent to $\\mathbf{w}^{\\mathbf{T}} \\phi = 0$, separates the two classes and the magnitude of $\\mathbf{w}$ goes to infinity. In this case, the **logistic sigmoid function** becomes **infinitely steep in feature space**, corresponding to a **Heaviside step function**, so that every training point from each class $k$ is assigned a posterior probability $p(\\mathcal{C}_k |x) = 1$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1 align=\"center\">End of Lecture</h1>"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
