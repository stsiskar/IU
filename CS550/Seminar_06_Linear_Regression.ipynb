{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "**Artificial Inteligence (CS550)**\n",
    "<br>\n",
    "Date: **12 February 2020**\n",
    "<br>\n",
    "Location: **SU, NEW STEM building**\n",
    "<br>\n",
    "Room: **304**\n",
    "\n",
    "Title: **Seminar ‚Ññ4**\n",
    "<br>\n",
    "Speaker: **Dr. Shota Tsiskaridze**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2 align=\"center\">Linear Regression</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3 align=\"center\">Land Price</h3>\n",
    "\n",
    "$\\bullet$ For simplicity, we consider the **toy linear regression problem**.\n",
    "\n",
    "$\\bullet$ For this we will use **synthetic data**.\n",
    "\n",
    "$\\textbf{Definition}$. **Synthetic data** is **any production data applicable to a given situation that are not obtained by direct measurement** according to the McGraw-Hill Dictionary of Scientific and Technical Terms.\n",
    "\n",
    "$\\bullet$ We are going to predict a **land price** (for simplicity, suppose its **quadratic**, i.e **X by X square** meters) given the **length of its side** (in meters).\n",
    "\n",
    "$\\bullet$ Let's assume that true **dependence** between the **length of land's side** and its **price** is given by **quadratic equation**:\n",
    "\n",
    "$$y = f(x) = a \\cdot x^2 + b \\cdot x + c > 0,$$\n",
    "\n",
    "where $x$ is the length of land's side and $y$ is its price.\n",
    "\n",
    "$\\bullet$ We also assume that our **observations are noisy**, and we model this noise by adding a **normally distributed (Gaussian)** term $\\varepsilon$ to our quadratic equation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Import the required Libraries\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "import sklearn.preprocessing as sk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let's introduce a method to generate some dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def generate_dataset(a, b, c, size: int, eps: float=2):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "    a, b, c: the coefficients of our quadratic equation\n",
    "    size:    size of the dataset\n",
    "    eps:     standard deviation (spread or \"width\") of the noise\n",
    "\n",
    "    Output:\n",
    "    X:       array of independent variables\n",
    "    y:       array of dependent variables\n",
    "    e:       error term, i.e. noise\n",
    "    \"\"\"\n",
    "    \n",
    "    # define a quadratic equation f(x)\n",
    "    f = lambda x: a*x**2 + b*x + c\n",
    "    \n",
    "    # define X axis\n",
    "    X = np.linspace(0.1, 10, num=size)\n",
    "    \n",
    "    # define Y axis\n",
    "    y = [f(i) for i in X]\n",
    "    \n",
    "    # generate the normally distributed noise\n",
    "    e = np.random.normal(0, eps, size=(size))\n",
    "    \n",
    "    return np.array(X), np.array(y), e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# define the true parameters for quadratic equation\n",
    "a = 4\n",
    "b = 1\n",
    "c = 2\n",
    "\n",
    "# generate the dataset\n",
    "X, y_true, e = generate_dataset(a, b, c, size=20, eps=10)\n",
    "\n",
    "# create true observations by adding noise\n",
    "y_obs = y_true + e\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.scatter(X, y_obs, s=20)\n",
    "plt.plot   (X, y_true, 'g')\n",
    "plt.xlabel(\"$x$ [meters]\",   fontsize=40)\n",
    "plt.ylabel(\"$y$ [currency]\", fontsize=40)\n",
    "plt.grid()\n",
    "#plt.savefig(\"p_original.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(X), type(y_obs), type(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "$\\bullet$ Let's select a method we want to fit the above dataset:\n",
    "\n",
    "$$\\hat{f_1}(x) = \\theta_0 + \\theta_1 \\cdot x.$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining linear regression model\n",
    "model = LinearRegression(fit_intercept=False)  #fit_intercept=False means disabling bias term"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "$\\bullet$ Therefore, our **regression models** involve the following **components**:\n",
    "- The **unknown parameters** denoted as a vector $\\theta$;\n",
    "- The **independent variables** denoted as an array $ùëã$.\n",
    "- The **dependent variable** denoted as an array $y$.\n",
    "- The **error term** defined as a noise and denoted as an array $e$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add dummy column for the bias\n",
    "X1 = np.c_[np.ones(X.shape[0]), X]\n",
    "X1[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fitting the model using Ordinary Least Square\n",
    "model.fit(X1, y_obs) \n",
    "\n",
    "# display learned weights: ùúÉ_0 and ùúÉ_1\n",
    "model.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate predictions\n",
    "y_pred = model.predict(X1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot learned model\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.scatter(X, y_obs, s=20)\n",
    "plt.plot   (X, y_true, 'g')\n",
    "plt.plot   (X, y_pred, 'r')\n",
    "plt.xlabel(\"$x$\", fontsize=40)\n",
    "plt.ylabel(\"$y$\", fontsize=40)\n",
    "plt.grid()\n",
    "#plt.savefig(\"p_1.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "$\\textbf{Question}$: Is it a **good** fit or **bad** fit, and **Why**?\n",
    "\n",
    "&ensp; Obviously, this model is too simple and doesn't have enough **power** to capture quadratic dependence.\n",
    "\n",
    "&ensp; Mathematically speaking, linear function isn't able to approximate quadratic one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "$\\bullet$ Let's introduce additional features to help model. We gonna use **polynomial features** up to **10-th**:\n",
    "\n",
    "$$\\hat{f_{10}}(x)=\\theta_0 + \\theta_1 \\cdot x + \\theta_2 \\cdot x^2 + \\theta_3 \\cdot x^3 + \\theta_4 \\cdot x^4 + \\theta_5 \\cdot x^5 + \\theta_6 \\cdot x^6 + \\theta_7 \\cdot x^7 + \\theta_8 \\cdot x^8 + \\theta_9 \\cdot x^9 +\\theta_{10} \\cdot x^{10}.$$\n",
    "\n",
    "$\\textbf{Note}$: despite the fact that we add polynomial features, **model stays linear** because linearity is relative to weights and not features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use **Scikit-Learn** library which helps with generating new features: \n",
    "\n",
    "`from sklearn.preprocessing import PolynomialFeatures`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the helper class\n",
    "poly = PolynomialFeatures(degree=10, include_bias=True)\n",
    "\n",
    "# create features\n",
    "X2 = poly.fit_transform(X.reshape(-1, 1))\n",
    "X2[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression(fit_intercept=False) \n",
    "\n",
    "model.fit(X2, y_obs) # fitting again with new feature set\n",
    "model.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "plt.scatter(X, y_obs, s=20)\n",
    "plt.plot   (X, y_true, 'g')\n",
    "plt.plot   (X, y_pred, 'r')\n",
    "plt.xlabel(\"$x$\", fontsize=40)\n",
    "plt.ylabel(\"$f(x)$\", fontsize=40)\n",
    "plt.grid()\n",
    "#plt.savefig(\"p_10.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "$\\textbf{Question}$: Is it a **good** fit or **bad** fit, and **Why**?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "$\\textbf{Excersice 1}$. **Generate several samples** and **fit the model**. Explain the observations!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Conclusions**:\n",
    "\n",
    " - **Very simple** models have **poor performance** due to lack of expressive power to learn data distribution.\n",
    " - **Very complex** models have **poor performance** due to excessive expressive power leading to **fitting noise** instead of the real data\n",
    "\n",
    "\n",
    "- In other words, the **first one** is **Underfit** and the **second one** is **Overfit** the our dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "$\\textbf{Excersice 2}$. **Write the code** that **fits** our data with **quadratic polynomial model**. Plot the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write me"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Let's for a given **single data point** $x_0$ calculate the following parameters using the **first model**:\n",
    " - **True Value** \n",
    " - **Predicted Value**\n",
    " - **Bias**\n",
    " - **Variance**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- For this, let's firts **write a method** that **generates** the dataset, **fits** it and returs the **true and predicted values**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_and_predict(x, poly):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "    x:        selected data point value\n",
    "    model:    selected model, or polynom\n",
    "\n",
    "    Output:\n",
    "    true_val: true value for the selected data point x\n",
    "    pred_val: predicted value\n",
    "    \"\"\"\n",
    "\n",
    "    # generate the dataset\n",
    "    X, y_true, e = generate_dataset(a, b, c, size=20, eps=10)\n",
    "\n",
    "    # create true observations by adding noise\n",
    "    y_obs = y_true + e \n",
    "        \n",
    "    # augment the dataset with selected model features\n",
    "    X1 = poly.fit_transform(X.reshape(-1, 1))\n",
    "\n",
    "    # define linear regression model\n",
    "    model = LinearRegression(fit_intercept=False) \n",
    "    \n",
    "    # fit the model using Ordinary Least Square\n",
    "    model.fit(X1, Y) \n",
    "    \n",
    "    true_val = f(x)\n",
    "    \n",
    "    return true_val, model.predict(poly.fit_transform([[x]]))[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "$\\textbf{Excersice 3}$. **Write the code** that trains model on **different datasets** and calculate **expected prediction** and it's **fluctuation**.\n",
    "\n",
    "In other words, you need to:\n",
    "\n",
    "- select the data point on which you will measure prediction quality and fluctuations:  `x0 = 5`\n",
    "- select the polynomial function, for example with 10th degree:`poly = ...`\n",
    "- train model on different datasets using the **generate_and_predict** method: `N = 1000`\n",
    "- save all the predicted values in an array: `y_preds`\n",
    "- get the **True Value**: `true_val`\n",
    "- get the **Average Estimate**: `mean_val`\n",
    "- get the **Bias**: `bias`\n",
    "- get the **Variance**: `variance`\n",
    "- fill the code below, print it and plot the distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# fill me\n",
    "true_val = 0\n",
    "mean_val = 0\n",
    "bias     = 0\n",
    "variance = 0  \n",
    "\n",
    "print('True Value: ',       true_val)\n",
    "print('Average Estimate: ', mean_val)\n",
    "print('Bias: ',             bias)\n",
    "print('Variance: ',         variance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fillme \n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(15, 5))\n",
    "sns.distplot(y_preds, ax=ax[0])\n",
    "ax[1].scatter([0]*len(y_preds), y_preds, s=0.1)\n",
    "ax[1].scatter([0], [y_true], s=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "$\\textbf{Excersice 4}$. **Plot** the similar **distributions** for the select the **first degree polynomial function**. Explain the both observations!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Let's add **regularization term** importing the **Ridge** and **Lasso** regressions from the Scikit-Learn library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge, Lasso\n",
    "\n",
    "# define the learning rate alpha (or lambda)\n",
    "alpha = 100\n",
    "\n",
    "# define linear regression model with regularization term\n",
    "model_ridge = Ridge(alpha=alpha, fit_intercept=False) \n",
    "model_lasso = Lasso(alpha=alpha, fit_intercept=False) \n",
    "\n",
    "poly = PolynomialFeatures(degree=6, include_bias=True)\n",
    "\n",
    "X2 = poly.fit_transform(X.reshape(-1, 1))\n",
    "\n",
    "# fit the model using Ordinary Least Square\n",
    "model_ridge.fit(X2, y_obs)\n",
    "model_lasso.fit(X2, y_obs)\n",
    "\n",
    "yr_pred = model_ridge.predict(X2)\n",
    "yl_pred = model_lasso.predict(X2)\n",
    "\n",
    "\n",
    "print(\"\\nL2 Norm of the Ridge model weights:\", np.sum(model_ridge.coef_**2))\n",
    "print('weights:')\n",
    "print('\\n'.join('ùúΩ%d=\\t%.3f' % (i, theta) for i, theta in enumerate(model_ridge.coef_)))\n",
    "\n",
    "print(\"\\nL2 Norm of the Lasso model weights:\", np.sum(model_lasso.coef_**2))\n",
    "print('weights:')\n",
    "print('\\n'.join('ùúΩ%d=\\t%.3f' % (i, theta) for i, theta in enumerate(model_lasso.coef_)))\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.scatter(X, y_obs, s=20)\n",
    "plt.plot   (X, y_true, 'g')\n",
    "plt.plot   (X, yr_pred, 'r')\n",
    "plt.plot   (X, yl_pred, 'b')\n",
    "plt.xlabel(\"$x$ [meters]\",   fontsize=40)\n",
    "plt.ylabel(\"$y$ [currency]\", fontsize=40)\n",
    "plt.grid()\n",
    "#plt.savefig(\"p_regularization.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "$\\textbf{Excersice 5}$. **Try** polynomial functions with the **different degrees**, for example `degree = 5`, `degree = 10`, `degree = 20`. Explain the observations!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1 align=\"center\">End of Seminar</h1>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
