{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "**Artificial Inteligence (CS550)**\n",
    "<br>\n",
    "Date: **24 February 2020**\n",
    "<br>\n",
    "Location: **SU, NEW STEM building**\n",
    "<br>\n",
    "Room: **302**\n",
    "\n",
    "Title: **Lecture 6: Linear Regression**\n",
    "<br>\n",
    "Speaker: **Dr. Shota Tsiskaridze**\n",
    "<br>\n",
    "Bibliography: \n",
    "<br>\n",
    "[1] Ian Goodfellow and Yoshua Bengio and Aaron Courville, *Deep Learning*, MIT Press, 2016."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1 align=\"center\">Linear Regression</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3 align=\"center\">Regression Model</h3>\n",
    "\n",
    "$\\bullet$ In practice, researchers:\n",
    "<br> &emsp; $\\bullet$ **select a model** they would like to estimate,\n",
    "<br> &emsp; $\\bullet$ **use chosen method** to estimate the parameters of that model.\n",
    " \n",
    "$\\bullet$ Regression models involve the following components:\n",
    "<br> &emsp; $\\bullet$ The **unknown parameter**s, often denoted as a scalar or vector $\\boldsymbol{\\theta}$.\n",
    "<br> &emsp; $\\bullet$ The **independent variables**, which are observed in data and are often denoted as a vector $X_i$.\n",
    "<br> &emsp; $\\bullet$ The **dependent variable**, which are observed in data and often denoted as a scalar $y_{i}$.\n",
    "<br> &emsp; $\\bullet$ The **error term**s, which are not directly observed in data and are often denoted as a scalar $\\varepsilon_i$.\n",
    "\n",
    "$\\bullet$ Regression models propose that $y_{i}$ is a function of $X_{i}$ and $\\boldsymbol{\\theta}$, with $\\varepsilon_i$ representing an **additive error term**:\n",
    "\n",
    "$$y_i = f(X_i, \\boldsymbol{\\theta}) + \\varepsilon_i.$$\n",
    "\n",
    "$\\bullet$ The researchers' **goal** is to **estimate the function** $f(X_i, \\boldsymbol{\\theta})$ that most closely fits the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<h3 align=\"center\">Multivariate Linear Regression Problem</h3>\n",
    "\n",
    "\n",
    "$\\bullet$ **Linear regression** model assumes that the relationship between the **dependent variable** $Y_i$ and the \n",
    "<br> &ensp; **independent variables** (**regressors**) $X_i$ is **linear**, i.e. the **regresion function** takes the form:\n",
    "\n",
    "$$f(X_i, \\boldsymbol{\\theta}) = \\theta_1 X_1 + \\theta_2 X_2 + \\cdots + \\theta_n X_n.$$\n",
    "\n",
    "$\\bullet$ Suppose we have $m$ training examples $(X_i, y_i)$ and $n$ features $X_i = [x_{i1}, x_{i2}, \\cdots, x_{in} ]^T\\in \\mathbb{R}^n$.\n",
    "\n",
    "$\\bullet$ We can represent $X_i$ as a **Design Matrix** $X = \\begin{bmatrix}\n",
    "x_{11} & x_{12} & \\cdots & x_{1n}\\\\\n",
    "x_{21} & x_{22} & \\cdots & x_{1n}\\\\ \n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "x_{m1} &  x_{m2} &\\cdots & x_{mn}\\\\\n",
    "\\end{bmatrix}$, and $y_i$ as a vector $\\mathbf{y} = \\begin{bmatrix}\n",
    "y_{1}\\\\ \n",
    "y_{2}\\\\ \n",
    "\\vdots\\\\\n",
    "y_{m}\n",
    "\\end{bmatrix}$.\n",
    "\n",
    "$\\bullet$ Thus we have a system:\n",
    "$$ X  \\boldsymbol{\\theta} = \\mathbf{y}.$$\n",
    "\n",
    "$\\bullet$ How do we solve it, and if there's no solution, how do we find the best possible $\\boldsymbol{\\theta}$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<h3 align=\"center\">Note</h3>\n",
    "\n",
    "$\\bullet$ Usially, there is an additional feature $x_{i0} = 1$ - the slope, so:\n",
    "\n",
    "$$\\boldsymbol{\\theta} = \n",
    "\\begin{bmatrix}\n",
    "\\theta_{0}\\\\ \n",
    "\\theta_{1}\\\\ \n",
    "\\vdots\\\\\n",
    "\\theta_{n}\n",
    "\\end{bmatrix}\n",
    "\\in \\mathbb{R}^{n+1}\n",
    "\\text{, } \n",
    "X_i = \n",
    "\\begin{bmatrix}\n",
    "x_{i0}\\\\ \n",
    "x_{i1}\\\\ \n",
    "\\vdots\\\\\n",
    "x_{in}\n",
    "\\end{bmatrix}\n",
    "\\in \\mathbb{R}^{n+1}\n",
    "\\text{, } \n",
    "\\mathbf{y} = \n",
    "\\begin{bmatrix}\n",
    "y_{1}\\\\ \n",
    "y_{2}\\\\ \n",
    "\\vdots\\\\\n",
    "y_{m}\n",
    "\\end{bmatrix}\n",
    "\\text{, and } \n",
    "X = \\begin{bmatrix}\n",
    "x_{10} & x_{11} & \\cdots & x_{1n}\\\\\n",
    "x_{20} & x_{21} & \\cdots & x_{1n}\\\\ \n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "x_{m0} &  x_{m1} &\\cdots & x_{mn}\\\\\n",
    "\\end{bmatrix}\n",
    "\\in \\mathbb{R}^{m\\times n + 1}.$$\n",
    "\n",
    "$\\bullet$  Thus we have a system: \n",
    "\n",
    "$$X  \\boldsymbol{\\theta} = \\mathbf{y}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3 align=\"center\">Residuals and Coefficient of Determination</h3>\n",
    "\n",
    "$\\bullet$ In **regression analysis**, the difference between the **observed value** of the dependent variable $\\mathbf{y}$ and \n",
    "<br> &ensp; the predicted value $\\hat{\\mathbf{y}}$ is called the **residual** $\\mathbf{e}$. Each data point has one residual:\n",
    "\n",
    "$$e_i = y_i - \\hat{y_i}.$$\n",
    "\n",
    "$\\bullet$ Conventionally the expectation value (**mean**) of the residuals is zero: $\\mathbf{E}(\\mathbf{e}) = 0$.\n",
    "\n",
    "$\\bullet$ What about $\\mathbf{Var}(\\mathbf{e})$?\n",
    "$$\\mathbf{Var}(\\mathbf{e}) = \\mathbf{Var}(\\mathbf{y})(1 - R^2).$$\n",
    "\n",
    "&ensp; where $R^2$ is called **coefficient of determination**:\n",
    "\n",
    "$$R \\equiv 1 - \\frac{SS_{res}}{SS_{tot}} \\equiv 1 - \\frac{\\sum_{i} e_i^2}{\\sum_{i}(y_i - \\bar{y})^2}$$\n",
    "\n",
    "$\\bullet$ It determines how much of the **total variation in $\\mathbf{y}$** is explained by the **variation in $X$**:\n",
    "<br> &emsp; $\\bullet$ $R^2=0$: the linear relationship **explains nothing** (so no linear relationship between $X$ and $\\mathbf{y}$);\n",
    "<br> &emsp; $\\bullet$ $R^2=1$: the linear relationship **explains everything** - no left-overs, no uncertainty.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<h3 align=\"center\">Least Squares Error</h3>\n",
    "\n",
    "$\\bullet$ When there's **no solution** to the system, we fit the data as good as possible.\n",
    "<br>\n",
    "$\\bullet$ We try to predict such $\\hat{\\mathbf{y}}$ that minimizes the residuals $\\mathbf{e}$.\n",
    "<br>\n",
    "$\\bullet$ One way of measuring the **performance of the model** is to compute the **MSE** of the residuals:\n",
    "\n",
    "$$J(\\boldsymbol{\\theta}) = \\| \\mathbf{e}\\|^2 = \\|\\mathbf{y} - \\hat{\\mathbf{y}} \\|^2$$\n",
    "$\\bullet$ So our task is to find $\\hat{\\boldsymbol{\\theta}}$:\n",
    "$$\\hat{\\boldsymbol{\\theta}} = \\arg\\min_{\\boldsymbol{\\theta}}J(\\boldsymbol{\\theta})$$\n",
    "$\\bullet$ Let's expand $J(\\boldsymbol{\\theta})$:\n",
    "\n",
    "$$J(\\boldsymbol{\\theta}) = \\| \\mathbf{y} - \\hat{\\mathbf{y}} \\|^2 = (\\mathbf{y} - X\\boldsymbol{\\theta})^T(\\mathbf{y} - X\\boldsymbol{\\theta}) = \\mathbf{y}^T\\mathbf{y} - 2\\boldsymbol{\\theta}^TX^T\\mathbf{y} + \\boldsymbol{\\theta}^TX^TX\\boldsymbol{\\theta}$$\n",
    "\n",
    "$\\bullet$ Use **Fermat's Theorem** to find the minimum:\n",
    "\n",
    "$$\\frac{\\partial J(\\boldsymbol{\\theta})}{\\partial \\boldsymbol{\\theta}} = -2X^T\\mathbf{y} + 2X^TX\\boldsymbol{\\theta} = 0 \\Rightarrow X^TX\\boldsymbol{\\theta} = X^T \\mathbf{y} \\Rightarrow \\hat{\\boldsymbol{\\theta}} = (X^TX)^{-1}X^T\\mathbf{y} = X^{+}\\mathbf{y},$$\n",
    "\n",
    "&ensp; where $X^{+} = (X^TX)^{-1}X^T$ is the **Pseudoinverse** of $X$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3 align=\"center\">System of Linear Equations</h3>\n",
    "\n",
    "$\\bullet$ In **Linear Algebra** we typically use **different notation**.\n",
    "<br>\n",
    "$\\bullet$ A general system of $m$ **linear equations** with $n$ **unknowns** can be written as:\n",
    "$$\\begin{matrix}\n",
    "a_{11} x_1 + a_{12} x_2 + \\cdots + a_{1n} x_n = b_1\\\\ \n",
    "a_{21} x_1 + a_{22} x_2 + \\cdots + a_{2n} x_n = b_2\\\\  \n",
    "\\vdots \\\\\n",
    "a_{m1} x_1 + a_{12} x_2 + \\cdots + a_{mn} x_n = b_m\n",
    "\\end{matrix}$$\n",
    "where $x_j$ are the unknowns, $a_{ij}$ are the coefficients of the system, and $b_i$ are the constant terms.\n",
    "<br>\n",
    "$\\bullet$ We can write this system of linear equations in the equivalent matrix form:\n",
    "\n",
    "$$A \\mathbf{x} = \\mathbf{b},$$\n",
    "\n",
    "where $A = \n",
    "\\begin{bmatrix}\n",
    "a_{11} & a_{12} & \\cdots & a_{1n}\\\\\n",
    "a_{21} & a_{22} & \\cdots & a_{1n}\\\\ \n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "a_{m1} & a_{m2} &\\cdots & a_{mn}\\\\\n",
    "\\end{bmatrix} \\in \\mathbb{R}^{m \\times n}$, \n",
    "$\\mathbf{x} = \\begin{bmatrix}\n",
    "x_{1}\\\\ \n",
    "x_{2}\\\\ \n",
    "\\vdots\\\\\n",
    "x_{n}\n",
    "\\end{bmatrix}  \\in \\mathbb{R}^{n}$ and $\\mathbf{b} = \\begin{bmatrix}\n",
    "b_{1}\\\\ \n",
    "b_{2}\\\\ \n",
    "\\vdots\\\\\n",
    "b_{m}\n",
    "\\end{bmatrix}  \\in \\mathbb{R}^{m}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3 align=\"center\">Column Space</h3>\n",
    "\n",
    "Let $\\mathcal{F}$ be a field and let $A$ be an $m \\times n$ matrix, with column vectors $\\mathbf{v}_1, \\mathbf{v}_2, \\cdots, \\mathbf{v}_n$, where $\\mathbf{v}_i \\in \\mathcal{F}^m$.\n",
    "\n",
    "$\\bullet$ The **set** of **all possible linear combinations** of column vectors $\\mathbf{v}_1, \n",
    "\\cdots, \\mathbf{v}_n$ is called the **column space**, $C(A)$:\n",
    "\n",
    "$$\\mathbf{v} = \\alpha_1 \\mathbf{v}_1 + \\cdots + \\alpha_n \\mathbf{v}_n,$$\n",
    "\n",
    "&ensp; where $\\alpha_1, ..., \\alpha_n \\in \\mathcal{F}$ are the scalars.\n",
    "\n",
    "$\\bullet$ Any linear combination of the column vectors can be written as the product of $A$ with a column vector:\n",
    "\n",
    "\n",
    "$$\\mathbf{v} = \n",
    "\\alpha_1\n",
    "\\begin{bmatrix}\n",
    "a_{11}\\\\ \n",
    "\\vdots\\\\\n",
    "a_{m1}\n",
    "\\end{bmatrix}\n",
    "+ \\cdots + \\alpha_n\n",
    "\\begin{bmatrix}\n",
    "a_{1n}\\\\ \n",
    "\\vdots\\\\\n",
    "a_{mn}\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "\\alpha_1 a_{11} + \\cdots + \\alpha_n a_{1n}\\\\\n",
    "\\alpha_1 a_{21} + \\cdots + \\alpha_n a_{2n}\\\\\n",
    "\\vdots \\\\\n",
    "\\alpha_1 a_{m1} + \\cdots + \\alpha_n a_{mn}\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "a_{11} & \\cdots & a_{1n}\\\\\n",
    "\\vdots & \\vdots & \\ddots\\\\\n",
    "a_{m1} & \\cdots & a_{mn}\\\\\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "\\alpha_{1}\\\\ \n",
    "\\vdots\\\\\n",
    "\\alpha_{n}\n",
    "\\end{bmatrix}\n",
    "= A \n",
    "\\begin{bmatrix}\n",
    "\\alpha_{1}\\\\ \n",
    "\\vdots\\\\\n",
    "\\alpha_{n}\n",
    "\\end{bmatrix}\n",
    "$$ \n",
    "\n",
    "$\\bullet$ Therefore, the **column space** of $A$ is the same as the **range** of the corresponding matrix **transformation**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3 align=\"center\">Projection onto Column Space</h3>\n",
    "\n",
    "Let's consider a system of linear equations in the matrix form $A \\mathbf{x} = \\mathbf{b}$.\n",
    "\n",
    "$\\bullet$ If $\\mathbf{b} \\notin C(A)$, then the system does not have a solution.\n",
    "\n",
    "$\\bullet$ We can find an **approximate solution** by projecting $\\mathbf{b}$ onto $C(A)$. Let's multiply both sides by $A^T$:\n",
    "\n",
    "$$A^TA \\mathbf{x} = A^T\\mathbf{b},$$\n",
    "&ensp; i.e. we've got the system of **Normal Equation**.\n",
    "\n",
    "$\\bullet$ We can find solution analytically, by simply solving the system of equations:\n",
    "\n",
    "$$\\hat{\\mathbf{x}} = (A^TA)^{-1}A^T \\mathbf{b}.$$\n",
    "\n",
    "$\\bullet$ $A^{+} = (A^TA)^{-1}A^T$ is also called the **Pseudoinverse** of the matrix $A$.\n",
    "\n",
    "$\\bullet$ It also happens to be the best solution in terms of **Least Squares error**:\n",
    "\n",
    "$$\\|\\mathbf{b} - A\\hat{\\mathbf{x}}\\|_2 =  \\min_{\\mathbf{x}}(\\|\\mathbf{b} - A\\mathbf{x}\\|_2)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3 align=\"center\">Invertability of $A^TA$</h3>\n",
    "\n",
    "$\\bullet$ When $A^TA$ is invertible?\n",
    "<br> &emsp; $\\bullet$ According the $\\textbf{Theorem} \\space \\textbf{11}$ from $\\textbf{Lecture} \\space \\textbf{2}$, the matrix $A^TA$ is invertible if $\\mathfrak{N}(A^TA) = \\{0\\}$;\n",
    "<br> &emsp; $\\bullet$ But $\\mathfrak{N}(A^TA) = \\mathfrak{N}(A)$, therefore the matrix $A^TA$ is invertible if $\\mathfrak{N}(A)=\\{0\\}$;\n",
    "<br> &emsp; $\\bullet$ In other words, the matrix $A^TA$ is **invertible** if the **column vectors** of $A$ are **linearly independent**.\n",
    "\n",
    "$\\bullet$ When does $A^TA$ have no inverse?\n",
    "<br> &emsp; $\\bullet$ similarly, $A^TA$ is not invertible if $\\mathfrak{R}(A^TA) = \\mathfrak{R}(A)$;\n",
    "<br> &emsp; $\\bullet$ In other words, some **columns** are **linearly dependent**;\n",
    "<br> &emsp; $\\bullet$ Or, for example, when we have too many features $(m < n)$.\n",
    "\n",
    "$\\bullet$ **Solution**:\n",
    "<br> &emsp; $\\bullet$ Remove the linear dependency;\n",
    "<br> &emsp; $\\bullet$ Delete some features.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3 align=\"center\">Gradient Descent vs Normal Equation</h3>\n",
    "\n",
    "|                                                      Gradient Descent                                                      |                                Normal Equation                                |\n",
    "|:---------------------------------------------------------------------------------------------------------------------------|:------------------------------------------------------------------------------|\n",
    "| **Has a hyperparameter** (learning rate $\\alpha$)                                                                              | **Does not have a hyperparameter**                                                  |\n",
    "| An **iterative process**, <br> i.e. many iterations are required  to achieve the required accuracy | **No iterations required**, i.e computed in one step                              |\n",
    "| **No inverse computation is required**                                                                                         | **Computation of the inverse matrix $X^TX$ is required**      |\n",
    "| Works well with **large number of features** $n$. <br>Time complexity is $O(nC\\log(1/\\varepsilon))$  for **GD**, and <br > Time complexity is $O(C/\\varepsilon)$ for **SGD**| **Very slow** if number of features $n$ is large: $n \\geq 10^4$. <br>Time complexity is $O(n^3)$ |\n",
    "| An iterative process **may not converge**                                                                                      | **May not have an analytical solution**, i.e. $X^TX$ is not-invertible               |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<h3 align=\"center\">Gradient Descent Learning Rate Problems</h3>\n",
    "\n",
    "$\\bullet$ If we **stack in local extrema** or saddle point, we can **stay there for long**;\n",
    "\n",
    "$\\bullet$ **Large learning rate** might cause so called **bouncing gradient**.\n",
    "\n",
    "<br>\n",
    "\n",
    "<center><img src=\"images/GD_0.png\" width=\"1800\" alt=\"Example\" /></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<h3 align=\"center\">Optimization Methods for Gradient Descent</h3>\n",
    "\n",
    "$\\bullet$ The following two animations provide some intuitions towards the behaviour of the optimization methods.\n",
    "<br>\n",
    "\n",
    "<center><img src=\"images/GD_Optimizers2.gif\" width=\"1800\" alt=\"Example\" /></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3 align=\"center\">Overfitting and Underfitting</h3>\n",
    "\n",
    "$\\bullet$ The central challenge in machine learning is that we must perform well on **previously unseen (new) inputs**.\n",
    "<br>\n",
    "$\\bullet$ The ability to perform well on previously unobserved inputs is called **generalization**.\n",
    "<br>\n",
    "$\\bullet$ There are two type of errors we can compute:\n",
    "<br> &emsp; $\\bullet$ error measure on the training set called the **training error**.\n",
    "<br> &emsp; $\\bullet$ **test error**, or **generalization error**, defined as the expected value of the error on a **new input**.\n",
    "\n",
    "$\\bullet$ In our linear regresion example, the **training** and **test error** are:\n",
    "\n",
    "$$\\text{training error} = \\frac{1}{m^{(train)}} || \\mathbf{X}^{(train)} - \\mathbf{y}^{(train)}||_2^2$$\n",
    "\n",
    "$$\\text{test error} = \\frac{1}{m^{(test)}} || \\mathbf{X}^{(test)} - \\mathbf{y}^{(test)}||_2^2$$\n",
    "\n",
    "$\\bullet$ The factors determining **how well** a **machine learning** algorithm will **perform** are its ability to:\n",
    "<br> &emsp; $\\bullet$ Make the **training error** small.\n",
    "<br> &emsp; $\\bullet$ Make the gap between **training** and **test error** small.\n",
    "<br>\n",
    "$\\bullet$ These factors correspond to the two central challenges in machine learning: **underfitting** and **overfitting**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<h3 align=\"center\">Capacity</h3>\n",
    "\n",
    "\n",
    "\n",
    "$\\bullet$ Model’s **capacity** is its ability to fit a wide variety of functions. \n",
    "<br> &ensp; It **can control** whether a model is more likely to **underfit** or **overfit**.\n",
    "\n",
    "$\\bullet$  One way to control the **capacity** of a ML algorithm is by choosing its **hypothesis space**, \n",
    "<br> &ensp; the set of functions that the learning algorithm is allowed to select as being the solution.\n",
    "\n",
    "<center><img src=\"images/Capacity.png\" width=\"1000\" alt=\"Example\" /></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<h3 align=\"center\">Bias and Variance of an Estimator</h3>\n",
    "\n",
    "\n",
    "-  **Bias** of an estimator is the **expected difference** between its **estimates** and the **true values** in the data:\n",
    "\n",
    "$$\\operatorname{Bias}(\\hat{f}(x_0))=f(x_0)-\\mathbf{E}\\left [\\hat{f}(x_0) \\right ]$$\n",
    "\n",
    " - **Variance** of an estimator is defined as the **expected value** of the **squared difference** between the **estimate** of a model and the **expected value** of the **estimate**:\n",
    "\n",
    "$$\\operatorname{Var}(\\hat{f}(x_0))=\\mathbf{E}\\left [ \\left(\\hat{f}(x_0)-\\mathbf{E} \\left[\\hat{f}(x_0)\\right] \\right)^2 \\right]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<h3 align=\"center\">Bias–Variance Decomposition</h3>\n",
    "\n",
    "- **Total expected error** a point $x_0$ is defined as follows:\n",
    "\n",
    "$$\\mathbf{E} \\left [ \\left ( f(x_0) - \\hat{f}(x_0)\\right )^2\\right ].$$\n",
    "\n",
    "- **Whichever model**  $\\hat{f}$ we choose, its expected error can be further **decomposed** as:\n",
    "\n",
    "$$\\mathbf{E} \\left [\\left(f(x_0) - \\hat{f}(x_0)\\right)^2\\right]\n",
    "= \\left(\\operatorname{Bias}\\left[\\hat{f}(x_0)\\right] \\right) ^2 + \\operatorname{Var}\\left[\\hat{f}(x_0)\\right] + \\sigma^2,$$\n",
    "\n",
    "&emsp; &emsp; &ensp;where $\\sigma^2$ is an **irreducible error** which we can't get rid off.\n",
    "\n",
    "- **Estimating expected error** is, in some sense, a **good way** to estimate model's **generalization ability**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<h3 align=\"center\">Bias-Variance Trade-Off</h3>\n",
    "\n",
    "$\\bullet$ Usually in **Machine Learning** there is a **trade-off** between model's **Bias** and **Variance**.\n",
    "\n",
    "$\\bullet$ As we gradually **grow model's capacity**, we gradually **reduce bias** and **increase variance**.\n",
    "\n",
    "$\\bullet$ The **goal** is to **find a sweet spot** where both bias and variance are acceptable.\n",
    "\n",
    "<center><img src=\"images/biasvariance.png\" width=\"800\" alt=\"Example\" /></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "$\\bullet$ Let's consider the dataset generated by the function $f(x) = 4x^2 + 3x + 2$.\n",
    "<br> \n",
    "$\\bullet$ True value is calculated at $x_0 = 5$.\n",
    "\n",
    "<center><img src=\"images/Polynomials3.png\" width=\"1500\" alt=\"Example\" /></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3 align=\"center\">Regularization</h3>\n",
    "\n",
    "$\\bullet$ **Regularization** is a technique used to **address overfitting**;\n",
    "<br>\n",
    "$\\bullet$ **Regularization**, significantly **reduces the variance** of the model, **without** substantial **increase in its bias**.\n",
    "<br>\n",
    "$\\bullet$ **Main idea** of regularization is to **keep all the features**, but **reduce magnitude of parameters** $\\boldsymbol{\\theta}$;\n",
    "$\\bullet$ A **regularization term** (or **regularizer**) $R(f)$ is added to a **loss function**:\n",
    "\n",
    "$$J_{reg}(\\boldsymbol{\\theta}) = J(\\boldsymbol{\\theta}) + R(f).$$\n",
    "\n",
    "$\\bullet$ Mainly, there are two types of forms of regularization:\n",
    "<br> &emsp; $\\bullet$ **Ridge regularization**, or **Tikhonov regularization**, uses he $L_2$ **norm** of the vector $\\boldsymbol{\\theta}$:\n",
    "$$R(f) = \\lambda \\cdot \\| \\boldsymbol{\\theta} \\|^2_2 = \\lambda \\cdot \\boldsymbol{\\theta}^T\\boldsymbol{\\theta} = \\lambda \\cdot \\sum_{i=1}^n \\theta_i^2$$\n",
    "&emsp; $\\bullet$ **Lasso regularization** uses he $L_1$ **norm** of the vector $\\boldsymbol{\\theta}$:\n",
    "$$R(f) = \\lambda \\cdot \\| \\boldsymbol{\\theta} \\|_1 = \\lambda \\cdot \\sum_{i=1}^n |\\theta_i|$$\n",
    "$\\bullet$ $\\lambda$ is a **hyperparameter**, called **regularization parameter**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3 align=\"center\">Regularization Parameter $\\lambda$</h3>\n",
    "\n",
    "$\\bullet$ We need to choose $\\lambda$ carefully: **large** $\\lambda$ will lead to **underfitting**.\n",
    "\n",
    "$\\bullet$ **Green** - True distribution, **Red** - Linear Regression (**LR**);\n",
    "<br> &ensp; **Blue** - LR with **Lasso** regularization, **Magenta** - LR with **Ridge** regularization.\n",
    "<center><img src=\"images/preg2.png\" width=\"1300\" alt=\"Example\" /></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3 align=\"center\">Regularization in case of Normal Equations</h3>\n",
    "\n",
    "$\\bullet$ As we expressed above, analytical solution of the Normal Equation is:\n",
    "\n",
    "$$\\hat{\\boldsymbol{\\theta}} = (X^TX)^{-1}X^T\\mathbf{y} = X^{+}\\mathbf{y}.$$\n",
    "\n",
    "$\\bullet$ We add **regularization term** as:\n",
    "\n",
    "$$\\hat{\\boldsymbol{\\theta}} = (X^TX + \\lambda E^{+})^{-1}X^T\\mathbf{y} = X^{+}\\mathbf{y},$$\n",
    "\n",
    "&ensp; where $E^{+} \\in \\mathbb{R}^{(n+1)\\times(n+1)}$ and is almost identity matrix:\n",
    "\n",
    "$$E^{+} = \\begin{bmatrix}\n",
    "0 & 0 & \\cdots & 0\\\\\n",
    "0 & 1 & \\cdots & 0\\\\ \n",
    "0 & \\vdots & \\ddots & \\vdots \\\\\n",
    "0 & 0 &\\cdots & 1\n",
    "\\end{bmatrix}.$$\n",
    "\n",
    "$\\bullet$ Matrix $X^TX + \\lambda E^{+}$ is **allways invertible**!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1 align=\"center\">End of Lecture</h1>\n",
    "\n",
    "<h1 align=\"center\">The Next will be a Midterm Exam №1</h1>"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
