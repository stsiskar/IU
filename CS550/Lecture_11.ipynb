{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Artificial Inteligence (CS550)**\n",
    "<br>\n",
    "Title: **Lecture 11: Neural Networs**\n",
    "<br>\n",
    "Speaker: **Dr. Shota Tsiskaridze**\n",
    "\n",
    "Bibliography: [1] Christopher M. Bishop, *Pattern Recognition and Machine Learning*, Springer, 2006."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1 align=\"center\">Neural Networks</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3 align=\"center\">Discussions</h3>\n",
    "\n",
    "- We saw that models for **regression and classification**, that comprised **linear combinations** of **fixed basis functions** have **useful analytical** and **computational properties**;\n",
    "\n",
    "- However, theit practical applicability was limited by the **curse of dimensionality**;\n",
    "\n",
    "- **Curse of dimensionality**: with a fixed number of training samples, the predictive power of a classifier or regressor first increases as number of dimensions or features used is increased but then decreases.\n",
    "\n",
    "\n",
    "- **Support Vector Machine** (**SVM**) address this by **first defining basis functions** that are **centred on the training data points** and **then selecting a subset** of these during training.\n",
    "- One **advantage** of **SVM** is that, although the training involves **nonlinear optimization**, the **objective function is convex**, and so the **solution** of the optimization problem is **relatively straightforward**.\n",
    "\n",
    "\n",
    "- **An alternative approach** is to **fix the number of basis functions** in advance but **allow** them to **be adaptive**;\n",
    "- **Adaptive** means to use **parametric forms for the basis functions** in which the **parameter values** are **adapted during training**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<h3 align=\"center\">Feed-forward Network Functions (1)</h3>\n",
    "\n",
    "- The **linear models** for **regression and classification** were **based** on based on **linear combinations of fixed nonlinear basis functions** $\\phi_j(\\mathbf{x})$:\n",
    "\n",
    "  $$y(\\mathbf{x}, \\mathbf{w}) = f \\left ( \\sum_{j=1}^{M} w_j \\phi_j (\\mathbf{x}) \\right ),$$\n",
    "  \n",
    "  where $f(\\cdot)$ is a **nonlinear activation function** in the case of **classification** and is the **identity** in the case of **regression**.\n",
    "\n",
    "\n",
    "- Our **goal** is to extend this model by **making the basis functions $\\phi_j(\\mathbf{x})$ depend on parameters** and then to **allow these parameters to be adjusted**, along with the coefficients $\\{w_j\\}$, during training.\n",
    "- There are, of course, **many ways** to construct **parametric nonlinear basis functions**. \n",
    "\n",
    "\n",
    "- **Neural networks** use **basis functions** that itself is a **nonlinear function of a linear combination of the inputs**.\n",
    "\n",
    "\n",
    "- Lets describe the basic neural network model.\n",
    "\n",
    "\n",
    "<img src=\"images/L11_DNN.jpg\" width=\"500\" alt=\"Example\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<h3 align=\"center\">Feed-forward Network Functions (2)</h3>\n",
    "\n",
    "Lets consider the **basic neural network model**, which can be described a series of functional transformations.\n",
    "\n",
    "- First we construct $M$ linear combinations of the input variables $\\mathbf{x}_1, \\mathbf{x}_2, ..., \\mathbf{x}_D$ in the form:\n",
    "\n",
    "  $$a_j = \\sum_{i=1}^{D} w_{ji}^{(1)}x_i + w_{j0}^{(1)},$$\n",
    "  \n",
    "  where $j = 1, ..., M$ and the **superscript (1)** indicates that the corresponding parameters are in the **first layer** of the network.\n",
    "  \n",
    "  $a_j$ are known as **activations**.\n",
    "  \n",
    "- Each of **activations** is then **transformed** using a **differentiable**, **nonlinear activation function** $h(\\cdot)$ to give:\n",
    "\n",
    "  $$z_j = h(a_j).$$\n",
    "  \n",
    "  $h(\\cdot)$, in the context of neural networks, are called **hidden units**.\n",
    "  \n",
    "  The **nonlinear functions** $h(\\cdot)$ are generally **chosen** to be **sigmoidal function**s such as the **logistic sigmoid** or the **tanh**.\n",
    "  \n",
    "  \n",
    "- Lets now use these values are again linearly combined to give **output unit activations**:\n",
    "\n",
    "  $$a_k = \\sum_{j=1}^{M} w_{kj}^{(2)}z_j + w_{k0}^{(2)}$$\n",
    "  \n",
    "  where $k = 1, ..., K$ and $K$ is the **total number of outputs**.\n",
    "  \n",
    "  This **transformation corresponds** to the **second layer** of the network.\n",
    "\n",
    "\n",
    "- Finally, the **output unit activations** are transformed using an **appropriate activation function** to give a **set of network outputs** $y_k$:\n",
    "\n",
    "\n",
    "- For standard **regression problems**, the activation function is the **identity** so that:\n",
    "\n",
    "\n",
    "  $$y_kk = a_k.$$\n",
    "   \n",
    "- For multiple binary **classification problems**, each **output unit activation** is transformed using a **logistic sigmoid function** so that:\n",
    "  \n",
    "  $$y_k = \\sigma(a_k),$$\n",
    "    \n",
    "  where\n",
    "    \n",
    "  $$\\sigma (a) = \\frac{1}{1 + \\mathrm{exp}(-a)}.$$\n",
    "  \n",
    "<img src=\"images/L11_Basic_Neural_Network.png\" width=\"500\" alt=\"Example\" />\n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<h3 align=\"center\">Multilayer Perceptron</h3>\n",
    "\n",
    "- We can combine the various stages and get **overall network function** (for example for sigmoidal output unit activation functions), that has form:\n",
    "\n",
    "  $$y_k(\\mathbf{x}, \\mathbf{w}) = \\sigma \\left ( \\sum_{j=1}^{M} w_{jk}^{(2)} h \\left( \\sum_{i=1}^{D} w_{ji}^{(1)}x_i + w_{j0}^{(1)}  \\right) w_{k0}^{(2)} \\right ).$$\n",
    "  \n",
    "  or, **absorbing** the bias parameters into the set of weight parameters by defining an additional input variable, the above expressions takes form:\n",
    "  \n",
    "  $$a_i = \\sum_{i=0}^{D} w_{ji}^{(1)} x_i,$$\n",
    "\n",
    "  and\n",
    "  \n",
    "  $$y_k(\\mathbf{x}, \\mathbf{w}) = \\sigma \\left ( \\sum_{j=0}^{M} w_{jk}^{(2)} h \\left( \\sum_{i=0}^{D} w_{ji}^{(1)}x_i   \\right) \\right ).$$\n",
    "\n",
    "\n",
    "- The **neural network model** comprises **two stages of processing**, each of which resembles the **perceptron model**.\n",
    "\n",
    "\n",
    "- For this reason the neural network is also known as the **multilayer perceptron (MLP)**.\n",
    "\n",
    "<img src=\"images/L11_Perceptron.png\" width=\"400\" alt=\"Example\" />\n",
    "\n",
    "\n",
    "- **A key difference** compared to the perceptron, is that the **neural network uses continuous sigmoidal nonlinearities in the hidden units**, whereas the **perceptron uses step-function nonlinearities**.\n",
    "\n",
    "- The **neural network function** is **differentiable** with respect to the **network parameters**. This property plays a central role in network training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<h3 align=\"center\">Skip-Layer Connections</h3>\n",
    "\n",
    "- Another generalization of the network architecture is to include **skip-layer connections**:\n",
    "\n",
    "- For instance, in a **two-layer network** these would **go directly from inputs to outputs**.\n",
    "\n",
    "<img src=\"images/L11_Skip_Layer.png\" width=\"500\" alt=\"Example\" />\n",
    "\n",
    "\n",
    "- Because there is a **direct correspondence between** a **network diagram** and its **mathematical function**, we can develop more **general network** mappings by **considering more complex network diagrams**.\n",
    "\n",
    "\n",
    "- However, these must be restricted to a **feed-forward architecture**, in other words to one having **no closed directed cycles**, to ensure that the **outputs are deterministic functions of the inputs**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<h3 align=\"center\">Capability of a Multilayer Perceptron</h3>\n",
    "\n",
    "- A **two-layer network** with linear outputs can uniformly **approximate any continuous function** on a compact input domain to arbitrary accuracy provided the network has a sufficiently large number of hidden units.\n",
    "\n",
    "- Below, a multilayer perceptron to approximate four different functions:\n",
    "  - (a) $f(x) = x^2$\n",
    "  - (b) $f(x) = \\sin(x)$\n",
    "  - (c) $f(x) = |x|$\n",
    "  - (d) $f(x) = H(x)$, where $H(x)$ is the **Heaviside** step function.\n",
    "  \n",
    "\n",
    "- The **resulting network functions** are shown by the **red curves**;\n",
    "- The **outputs** of the 3 **hidden units** are shown by the three **dashed curves**.\n",
    "\n",
    "<img src=\"images/L11_Capability_of_MLP.png\" width=\"700\" alt=\"Example\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<h3 align=\"center\">Network Training</h3>\n",
    "\n",
    "- **How to determine the network parameters**? <br> Same as for polynomial curve fitting: **minimize a sum-of-squares error function**.\n",
    " \n",
    "\n",
    "- Lets given a training set comprising a set of **input vectors** $\\{\\mathbf{x}_n\\}$, where $n = 1, ..., N$, together with a corresponding set of target vectors $t_n$ we minimize the error function:\n",
    "\n",
    "  $$E_{\\mathbf{w}} = \\frac{1}{2} \\sum_{n=1}^{N} || y(\\mathbf{x}_n, \\mathbf{w}) - t_n||^2.$$\n",
    "  \n",
    "  \n",
    "- Lets assume that $t$ **has a Gaussian distribution**:\n",
    "\n",
    "  $$p(t | \\mathbf{x, w}) = \\mathcal{N}(t | y(\\mathbf{x,w}), \\beta^{-1}),$$\n",
    " \n",
    "  where $\\beta$ is the **precision** (inverse variance) of the **Gaussian noise**.\n",
    "  \n",
    "  \n",
    "- If **input vectors** are **identically distributed observations** $\\mathbf{X} = \\{\\mathbf{x}_1, ..., \\mathbf{x}_n\\}$, along with corresponding target values $\\mathbf{t} = \\{t_1, ..., t_n\\}$, then:\n",
    "\n",
    "  $$p(\\mathbf{t}| \\mathbf{X}, \\mathbf{w}, \\beta) = \\prod_{n=1}^{N} p(t_n | \\mathbf{x_n}, \\mathbf{w}, \\beta).$$\n",
    "  \n",
    "  Taking the negative logarithm, we obtain the error function:\n",
    "  \n",
    "  $$\\frac{\\beta}{2} \\sum_{n=1}^{N} \\{y(\\mathbf{x}_n , \\mathbf{w}) - t_n\\}^2 - \\frac{N}{2} \\ln \\beta + \\frac{N}{2} \\ln (2\\pi),$$\n",
    "  \n",
    "  which **can be used** to **learn the parameters** $\\mathbf{w}$ and $\\beta$.\n",
    "  \n",
    "\n",
    "- The value of $\\mathbf{w}$ can be found by minimizing the $E(\\mathbf{w})$.\n",
    "- Lets denote the solution by $\\mathbf{w}_{\\mathrm{ML}}$, because **it corresponds to the maximum likelihood solution**.\n",
    "\n",
    "- Having found $\\mathbf{w}_{\\mathrm{ML}}$, the value of $\\beta$ **can be found** by **minimizing the negative log likelihood** to give:\n",
    "\n",
    "  $$\\frac{1}{\\beta_{\\mathrm{ML}}} = \\frac{1}{N} \\sum_{n=1}^{N} \\{ y(\\mathbf{x}_n, \\mathbf{w}_{\\mathrm{ML}}) - t_n\\}^2.$$\n",
    "  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<h3 align=\"center\">Parameter Optimization</h3>\n",
    "\n",
    "- How to finding a **weight vector** $\\mathbf{w}$ which **minimizes the chosen function** $E(\\mathbf{w})$?\n",
    "\n",
    "  Because the error $E(\\mathbf{w})$ is a smooth continuous function of $\\mathbf{w}$ we can we can equate the gradient to zero:\n",
    "  \n",
    "  $$\\nabla E(\\mathbf{w}) = 0$$\n",
    "\n",
    "- Clearly there is **no hope** of **finding an analytical solution** to this equation.\n",
    "\n",
    "\n",
    "- Therefore we use **sequential algorithms** such as **gradient decsent method**, or **stochastic gradient descent method**.\n",
    "\n",
    "\n",
    "- **Note**: there may be (almost allways) **multiple stationary points** and in particular **multiple minima**.\n",
    "\n",
    "<img src=\"images/L11_Multiple_OP.png\" width=\"500\" alt=\"Example\" />\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<h3 align=\"center\">Error Backpropagation</h3>\n",
    "\n",
    "- Lets find an **efficient technique** for **evaluating the gradient of an error function $E(\\mathbf{w})$** for a **feed-forward neural network**.\n",
    "\n",
    "\n",
    "- This technique is using a **local message passing scheme** in which **information** is **sent** alternately **forwards** and **backwards** through the network and is known as **error backpropagation**.\n",
    "\n",
    "\n",
    "- Most **training algorithms** involve an **iterative procedure** for **minimization of an error function**, with adjustments to the weights being made in a **sequence of steps**.\n",
    "- At **each step**, we can distinguish between **two distinct stages**:\n",
    "  - In the **first stage**, the **derivatives of the error function** with respect to the weights **must be evaluated**.\n",
    "  - In the **second stage**, the **derivatives** are **used to compute** the **adjustments** to be made to the **weights**. \n",
    "  \n",
    "  \n",
    "- Many **error functions** of practical interest, for instance those defined by maximum likelihood for a set of **i.i.d. data**, **comprise a sum of terms**, **one for each data point** in the training set, so that:\n",
    "\n",
    "  $$E(\\mathbf{w}) = \\sum_{n=1}^{N} E_n(\\mathbf{w}).$$\n",
    "  \n",
    "  \n",
    "- Lets consider first a **simple linear model** in which the outputs $y_k$ are linear combinations of the input variables $\\mathbf{x}_i$ so that:\n",
    "\n",
    "  $$y_k = \\sum_{n=1}^{N} w_{ki} x_i.$$\n",
    "  \n",
    "  An error function takes the form:\n",
    "  \n",
    "  $$E_n = \\frac{1}{2} \\sum_{k} (y_k(\\mathbf{x}_n, \\mathbf{w}) - t_{nk})^2 = \\frac{1}{2} \\sum_{k} (y_{nk} - t_{nk})^2.$$\n",
    "  \n",
    "- The **gradient of an error function** with respect to a weight $w_{ji}$ is given by:\n",
    "\n",
    "  $$\\frac{\\partial E_n}{\\partial w_{ji}} = (y_{nj} - t_{nj}) x_{ni}.$$\n",
    "  \n",
    "  \n",
    "- In a **general feed-forward network**, each **unit** computes a weighted sum of its inputs of the form:\n",
    "\n",
    "  $$a_j = \\sum_{i} w_{ji} z_i,$$\n",
    "  \n",
    "  where $z_i$ is the **activation of a unit**, that **sends a connection to unit $j$**, and $w_{ji}$ is the **weight associated with that connection**.\n",
    "  \n",
    "\n",
    "- Note that $E_n$ **depends on the weight** $w_{ji}$ **only via the summed input** $a_j$ to unit $j$, therefore we can write:\n",
    "\n",
    "  $$\\frac{\\partial E_n}{ \\partial {w_{ji}}} = \\frac{\\partial E_n}{\\partial a_j} \\frac{\\partial a_j}{\\partial w_{ji}}.$$\n",
    "  \n",
    "  Lets introduce a useful notation:\n",
    "  \n",
    "  $$\\delta_j \\equiv \\frac{\\partial E_n}{\\partial a_j}.$$\n",
    "  \n",
    "  Using:\n",
    "  \n",
    "  $$\\frac{\\partial a_j}{\\partial w_{ji}} = z_i$$\n",
    "  \n",
    "  we can finally obtain:\n",
    "  \n",
    "  $$\\frac{\\partial E_n}{\\partial w_{ji}} = \\delta_j z_i.$$\n",
    "  \n",
    "- Thus, **in order to evaluate the derivatives**, we need **only to calculate the value of $\\delta_j$** for each **hidden and output** unit in the network.\n",
    "\n",
    "- For the output units we have:\n",
    "\n",
    "  $$\\delta_k = y_k - t_k$$.\n",
    "\n",
    "- For the hidden units we have:\n",
    "\n",
    "  $$\\delta_j \\equiv \\frac{\\partial E_n}{\\partial a_j} = \\sum_{k} \\frac{\\partial E_n}{\\partial a_k} \\frac{\\partial a_k}{\\partial a_j},$$\n",
    "  \n",
    "  where the **sum** runs **over all units** $k$ **to which unit $j$ sends connections**.\n",
    " \n",
    "\n",
    "- Finally, we obtain the following **backpropagation formula**:\n",
    "\n",
    "  $$\\delta_j = h'(a_j) \\sum_{k} w_{kj}\\delta_k$$\n",
    "  \n",
    "  which tells us that the **value of $\\delta$** for a particular hidden unit **can be obtained by propagating the $\\delta$'s backwards** from units higher up in the network.\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<h3 align=\"center\">The Backpropagation Procedure</h3>\n",
    "\n",
    "<img src=\"images/L11_Hidden_Layer.png\" width=\"300\" alt=\"Example\" />\n",
    "\n",
    "\n",
    "The **backpropagation procedure** can be summarized as follows:\n",
    "\n",
    "- Apply an input vector $x_n$ to the network and forward propagate through the network using:\n",
    "\n",
    "  $$a_j = \\sum_{i} w_{ji} z_i \\textrm{ and } z_j = h(a_j)$$\n",
    "  to find the **activations** of all the **hidden** and **output units**.\n",
    "\n",
    "- Evaluate the $\\delta_k$ for all the **output units** using:\n",
    "\n",
    "  $$\\delta_k = y_k - t_k$$\n",
    " \n",
    "- Backpropagate the $\\delta$'s using:\n",
    "\n",
    "  $$\\delta_j = h'(a_j) \\sum_{k} w_{kj}\\delta_k$$\n",
    "  to obtain $\\delta_j$ for each **hidden unit** in the network.\n",
    "  \n",
    "- Finally, to **evaluate the required derivatives**, use:\n",
    "\n",
    "  $$\\frac{\\partial E_n}{\\partial w_{ji}} = \\delta_j z_i.$$\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<h3 align=\"center\">A Simple Example</h3>\n",
    "\n",
    "- Lets assume that the **hidden units** have **logistic sigmoid activation functions** given by:\n",
    "\n",
    "  $$h(a) \\equiv \\mathrm{tahn}(a),$$\n",
    "  \n",
    "  where \n",
    "  \n",
    "  $$\\mathrm{tahn} = \\frac{e^{a} - e^{-a}}{e^{a} + e^{-a}}.$$\n",
    "  \n",
    "  A useful feature of this function is:\n",
    "\n",
    "  $$h'(a) = 1 - h(a)^2$$<br>\n",
    "  \n",
    "  \n",
    "- Lets Considering the **standard sum-of-squares error function**:\n",
    "  \n",
    "  $$E_n = \\frac{1}{2} \\sum_{k=1}^{K} (y_k - t_k)^2$$\n",
    "\n",
    "  where $y_k$ is the **activation of output unit** $k$, and $t_k$ is the corresponding target, for a particular input pattern $\\mathbf{x}_n$.\n",
    "\n",
    "- **For each pattern** in the training set in turn, we:\n",
    "\n",
    "  - **perform a forward propagation** using:\n",
    "    $$a_j = \\sum_{i=0}^{D} w_{ji}^{(1)}x_i$$,\n",
    "    $$z_j = \\mathrm{tahn}(a_j)$$,\n",
    "    $$y_k = \\sum_{j=0}^{M} w_{kj}^{(2)} z_j$$.\n",
    "\n",
    "  - **compute the $\\delta_k$** for **each output unit** using:\n",
    "    \n",
    "    $$\\delta_k = y_k - t_k$$<br>\n",
    "  \n",
    "  - **backpropagate** these to **obtain $\\delta_j$** for the **hidden units** using:\n",
    "  \n",
    "    $$\\delta_j = (1 - z_j^2) \\sum_{k=1}^{K} w_{kj}\\delta_k.$$<br>\n",
    "    \n",
    "  - **obtain the derivatives** with respect to the **first-layer** and **second-layer**:\n",
    "  \n",
    "    $$\\frac{\\partial E_n}{\\partial w_{ji}^{(1)}} = \\delta_j x_i,$$\n",
    "    $$\\frac{\\partial E_n}{\\partial w_{kj}^{(2)}} = \\delta_k z_j.$$\n",
    "  \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "<h1 align=\"center\">End of Lecture</h1>"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
